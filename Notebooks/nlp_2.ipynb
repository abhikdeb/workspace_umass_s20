{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zK3cUsqPwsMl"
   },
   "source": [
    "# Homework 2, CS585 Fall 2019 (due Mar 6th, 2020)\n",
    "\n",
    "### **This HW is due on Mar 6th, 2020, submitted via Gradescope as a PDF (File > Print > Save as PDF). 150 points total.**\n",
    "\n",
    "#### IMPORTANT: After copying this notebook to your Google Drive, please add a link to it below. To get a publicly-accessible link, hit the *Share* button at the top right, then click \"Get shareable link\" and copy over the result. If you fail to do this, you will receive no credit for this homework!\n",
    "LINK: https://colab.research.google.com/drive/1n6U1fth6kahl4B7e6BP0fZ3j6cAzhNlp\n",
    "\n",
    "##### How to do this problem set:\n",
    "\n",
    "- Most of these questions require writing Python code and computing results, and the rest of them have textual answers. To generate the answers, you will have to fill out all code-blocks that say `IMPLEMENT ME` or `ENTER CODE HERE`.\n",
    "\n",
    "- For all of the textual answers you have to fill out have placeholder text which says  `Answer in one or two sentences here` or `YOUR ANSWER HERE` For each question, you need to replace the placeholder text with your answer.\n",
    " \n",
    "- The neural language model may take up to 10 minutes to train and the allenNLP models can take up to 30 minutes to finetune, so **start early**! The rest of the cells are designed so that you can run in a few minutes of computation time. If it is taking longer than that, you probably have made a mistake in your code.\n",
    "\n",
    "##### How to submit this problem set:\n",
    "- Write all the answers in this ipython notebook. Once you are finished (1) Generate a PDF via (File -> Print -> Save as PDF) and upload to Gradescope.\n",
    "  \n",
    "- **Important:** check your PDF before you turn it in to Gradescope to make sure it exported correctly. If Colab gets confused about your syntax, it will sometimes terminate the PDF creation routine early.\n",
    "\n",
    "- When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. Then you'll be sure it's actually right. One handy way to do this is by clicking `Runtime -> Run All` in the notebook menu.\n",
    "\n",
    "##### Academic honesty \n",
    "\n",
    "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. See the course page for honesty policies.\n",
    "\n",
    "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1L3qaTVCClZ"
   },
   "source": [
    "## Q0. Relu Dead Neuron (20 points)\n",
    "Relu is an activation funciton commonly used in many neural network architectures. Unfortunately, Relu units can be fragile at times as in the following scenario. Please complete the following question:\n",
    "\n",
    "[INLP ch.\\ 3, \\#8, all of (a), (b), and (c).]\n",
    "\n",
    "(You can insert mathematical symbols by using the double dollar sign \"$$\" just like in latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-D827FNC_W9"
   },
   "source": [
    "Considering the following two layer feed forward network whose output is $y$ and $x\\in \\{0,1\\}^D$.\n",
    "$$z_i\\ =\\ \\text{ReLU}(\\theta^{(x\\rightarrow z)}_i.x\\ +\\ b_i)$$\n",
    "$$y\\ =\\ \\theta^{(z\\rightarrow y)}.z$$\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "**a)** \n",
    "\n",
    "A node $z_i$ is considered dead when the gradient becomes zero during training and hence no further updates happen during backpropagation which affects the nodes beyond it. This is when the input to the ReLU becomes negative as the gradient of ReLU function is 0 when the input to it is negative.\n",
    "\n",
    "We can write the ReLU function and its gradient as below:\n",
    "\n",
    "$$f(x) = \\left \\{ \\begin{array}{rcl} 0 & \\mbox{for} & x < 0\\\\ x & \\mbox{for} & x \\ge 0\\end{array} \\right.  \\ \\ \\ \\ \\&\\ \\ \\ \\ f'(x) = \\left \\{ \\begin{array}{rcl} 0 & \\mbox{for} & x < 0\\\\ 1 & \\mbox{for} & x > 0\\end{array} \\right.$$\n",
    "\n",
    "\n",
    "Hence from our loss function, we get the below,\n",
    "\n",
    "$$\\left(\\theta^{(x\\rightarrow z)}_i.x\\ +\\ b_i\\right)\\ <\\ 0\\ \\ \\ \\implies \\theta^{(x\\rightarrow z)}_i.x\\ <\\ - b_i$$\n",
    "\n",
    "Since we know that $x\\ \\in\\ \\{0,1\\}^D$, max($x$) would be 1 element wise. Thus we can bring a stricter bound for the neurron to be dead as below,\n",
    "\n",
    "$$\\sum^D_{j=1} \\theta^{(x\\rightarrow z)}_{i,j}\\times\\ x_j\\ <\\ -b_i\\ \\implies\\ \\sum^D_{j=1} \\theta^{(x\\rightarrow z)}_{i,j}\\ <\\ -b_i$$\n",
    "\n",
    "\n",
    "**b)** \n",
    "\n",
    "Given that $\\frac{\\partial \\mathcal{l}}{\\partial y} = 1$,\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial b_i}\\ =\\ \\frac{\\partial l}{\\partial y}.\\frac{\\partial y}{\\partial z_i}.\\frac{\\partial z_i}{\\partial b_i}\\ \\ \\ \\ \\&\\ \\ \\ \\ \\frac{\\partial l}{\\partial \\theta^{(x\\rightarrow z)}_{j,i}}\\ =\\ \\frac{\\partial l}{\\partial y}.\\frac{\\partial y}{\\partial z_i}.\\frac{\\partial z_i}{\\partial \\theta^{(x\\rightarrow z)}_{j,i}}$$\n",
    "\n",
    "From the above, we derive the following to help achieve our objective:\n",
    "\n",
    "$$\\frac{\\partial z_i}{\\partial b_i}\\ =\\ \\delta\\left[\\theta^{(x\\rightarrow z)}_i.x\\ +\\ b_i\\right]\\ \\ \\ \\ \\ ; \\ \\ \\ \\frac{\\partial y}{\\partial z_i}\\ =\\ \\theta^{(z\\rightarrow y)}\\ \\ \\ \\ \\ ; \\ \\ \\ \\frac{\\partial z_i}{\\partial \\theta^{(x\\rightarrow z)}_{j,i}}\\ =\\ x_j.\\delta\\left[\\theta^{(x\\rightarrow z)}_i.x\\ +\\ b_i\\right]$$\n",
    "\n",
    "$$$$\n",
    "\n",
    "where,\n",
    "$$\\delta[\\phi] =\n",
    "    \\begin{cases}\n",
    "            1, &         \\text{if } \\phi > 0,\\\\\n",
    "            0, &         \\text{if } \\phi < 0.\n",
    "    \\end{cases}$$\n",
    "\n",
    "Hence substituting in the equations we get,\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial b_i}\\ =\\ \\theta^{(z\\rightarrow y)}_i.\\delta\\left[\\theta^{(x\\rightarrow z)}_i.x\\ +\\ b_i\\right]$$\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\theta^{(x\\rightarrow z)}_{j,i}}\\ =\\ \\theta^{(z\\rightarrow y)}.x_j.\\delta\\left[\\theta^{(x\\rightarrow z)}_i.x\\ +\\ b_i\\right]$$\n",
    "\n",
    "\n",
    "**c)** \n",
    "\n",
    "As can be seen in part (b), gradient becomes 0 when,\n",
    "\n",
    "$$\\theta^{(x\\rightarrow z)}_i.x\\ +\\ b_i\\ <\\ 0$$\n",
    "\n",
    "Hence, in a gradient based learning where we update the parameters ($\\theta^{x\\rightarrow z}, \\theta^{z\\rightarrow y}$) by the following:\n",
    "\n",
    "\n",
    "$$ \\theta^{x\\rightarrow z}_{next} \\leftarrow \\theta^{x\\rightarrow z}_{prev} + \\lambda_1.\\frac{\\partial l}{\\partial \\theta^{x\\rightarrow z}}\\ \\ \\ \\ ;\\ \\ \\ \\theta^{z\\rightarrow y}_{next} \\leftarrow \\theta^{z\\rightarrow y}_{prev} + \\lambda_2.\\frac{\\partial l}{\\partial \\theta^{z\\rightarrow y}}$$\n",
    "\n",
    "\n",
    "As the gradients become zero the parameter values get stagnant and the corresponding gradient also remains at 0 and hence no learning can further occur through that neuron and is hence considered dead as it will not affect the training of the model parameters irrespective of the further inputs it reads.\n",
    "\n",
    "\n",
    "\n",
    "Also from part (a), if the strict upper bound is breached, irrespective of the input to the model, the neuron will never be activated due to a 0 gradient which will make the neuron and succeeding neurons dead throughout its tenure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-nnjXfGDl6w"
   },
   "source": [
    "## Q1. N-gram Language model (40pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CS8DgBtkBvA2"
   },
   "source": [
    "To get started with the rest of the assignment, first run the following cell to create a PyDrive client and download data to your own Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5Cz0lbXL2_19",
    "outputId": "24ca0bd8-8fb4-4ff1-cec2-7e2d75c7ac64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8bZDdek4mo2"
   },
   "outputs": [],
   "source": [
    "import torch, pickle, os, sys, random, time\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from collections import *\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVKsHApr4zkD"
   },
   "outputs": [],
   "source": [
    "# Download wikitext02 data\n",
    "data = {'test': '', 'train': '', 'valid': ''}\n",
    "f_id = {'wiki.test.tokens': '1T8yMDqCqrWVXYJiTqKe2q8hoyAD_rV84', \n",
    "        'wiki.train.tokens': '1itIse_TpmFq1I5y7mxuXtzljnRwoM8gF', \n",
    "        'wiki.valid.tokens': '1K3SV5p8gu19DgQZtRvCViGMb26NJ1scd'}\n",
    "\n",
    "for fname in f_id:\n",
    "  f = drive.CreateFile({'id' : f_id[fname]})\n",
    "  f.GetContentFile(fname)\n",
    "  with open(fname, 'r') as f_wiki:\n",
    "    data[fname.split('.')[1]] = f_wiki.read().lower().split()\n",
    "\n",
    "vocab = list(set(data['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzRJcts-xirF"
   },
   "source": [
    "Now have a look at the data by running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "GLE1v7mRIKt-",
    "outputId": "0b3e3b3c-f90a-4869-cd40-427eaa4d816d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : ['=', 'valkyria', 'chronicles', 'iii', '=', 'senjō', 'no', 'valkyria', '3', ':'] ...\n",
      "dev : ['=', 'homarus', 'gammarus', '=', 'homarus', 'gammarus', ',', 'known', 'as', 'the'] ...\n",
      "test : ['=', 'robert', '<unk>', '=', 'robert', '<unk>', 'is', 'an', 'english', 'film'] ...\n",
      "first 10 words in vocab: ['bethel', 'byblos', 'karen', 'dimension', 'branching', 'sachs', 'board', 'friary', 'heart', 'function']\n"
     ]
    }
   ],
   "source": [
    "print('train : %s ...' % data['train'][:10])\n",
    "print('dev : %s ...' % data['valid'][:10])\n",
    "print('test : %s ...' % data['test'][:10])\n",
    "print('first 10 words in vocab: %s' % vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TZ91yDrl1Oq"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhEuT7w5ClUg"
   },
   "source": [
    "\n",
    "### **Q1.1:** Train N-gram language model (15pts)\n",
    "\n",
    "Complete the following *train_ngram_lm* function based on the following input/output specifications. If you've done it right, you should pass the tests in the cell below.\n",
    "\n",
    "*Input:*\n",
    "+ **data**: the data object created in the cell above that holds the tokenized Wikitext data\n",
    "+ **order**: the order of the model (i.e., the \"n\" in \"n-gram\" model) If order=3, we compute $p(w_2 | w_0, w_1)$.\n",
    "\n",
    "*Output:*\n",
    "+ **lm**: A dictionary where the key is the history and the value is a probability distribution over the next word computed using the maximum likelihood estimate from the training data. Importantly, this dictionary should include *backoff* probabilities as well; e.g., for order=4, we want to store $p(w_3 | w_0,w_1,w_2)$ as well as $p(w_3|w_1,w_2)$ and $p(w_3|w_2)$. \n",
    "\n",
    "Each key should be a single string where the words that form the history have been concatenated using spaces. Given a key, its corresponding value should be a dictionary where each word type in the vocabulary is associated with its probability of appearing after the key. For example, the entry for the history 'w1 w2' should look like:\n",
    "\n",
    "    \n",
    "    lm['w1 w2'] = {'w0': 0.001, 'w1' : 1e-6, 'w2' : 1e-6, 'w3': 0.003, ...}\n",
    "    \n",
    "In this example, we also want to store `lm['w2']` and `lm['']`, which contain the bigram and unigram distributions respectively.\n",
    "\n",
    "*Hint:* You might find the **defaultdict** and **Counter** classes in the **collections** module to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S13ismv99-N1"
   },
   "outputs": [],
   "source": [
    "def train_ngram_lm(data, order=3):\n",
    "    \"\"\"\n",
    "        Train n-gram language model\n",
    "    \"\"\"\n",
    "    # pad (order-1) special tokens to the left\n",
    "    # for the first token in the text\n",
    "    order -= 1\n",
    "    data = ['<S>'] * order + data # \n",
    "    lm = defaultdict(Counter)\n",
    "    for i in range(len(data) - order):\n",
    "      words = data[i:i+order+1]\n",
    "      word = data[i+order]\n",
    "      for j in range(0,order+1,1):\n",
    "        sub_key = ' '.join(words[j:-1])\n",
    "        lm[sub_key][word] += 1\n",
    "    for ki in lm.keys():\n",
    "      tot = sum(lm[ki].values())\n",
    "      for ski in lm[ki].keys():\n",
    "        lm[ki][ski] /= tot\n",
    "    return lm        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "QK0QYLxd49x3",
    "outputId": "a7fd128b-9577-4134-b05a-57377d93f858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking empty history ...\n",
      "checking probability distributions ...\n",
      "checking lengths of histories ...\n",
      "checking word distribution values ...\n",
      "Congratulations, you passed the ngram check!\n"
     ]
    }
   ],
   "source": [
    "def test_ngram_lm():\n",
    "  \n",
    "    print('checking empty history ...')\n",
    "    lm1 = train_ngram_lm(data['train'], order=1)\n",
    "    assert '' in lm1, \"empty history should be in the language model!\"\n",
    "    \n",
    "    print('checking probability distributions ...')\n",
    "    lm2 = train_ngram_lm(data['train'], order=2)\n",
    "    sample = [sum(lm2[k].values()) for k in random.sample(list(lm2), 10)]\n",
    "    assert all([a > 0.999 and a < 1.001 for a in sample]), \"lm[history][word] should sum to 1!\"\n",
    "    \n",
    "    print('checking lengths of histories ...')\n",
    "    lm3 = train_ngram_lm(data['train'], order=3)\n",
    "    assert len(set([len(k.split()) for k in list(lm3)])) == 3, \"lm object should store histories of all sizes!\"\n",
    "    \n",
    "    print('checking word distribution values ...')\n",
    "    assert lm1['']['the'] < 0.064 and lm1['']['the'] > 0.062 and \\\n",
    "           lm2['the']['first'] < 0.017 and lm2['the']['first'] > 0.016 and \\\n",
    "           lm3['the first']['time'] < 0.106 and lm3['the first']['time'] > 0.105, \\\n",
    "           \"values do not match!\"\n",
    "    \n",
    "    print(\"Congratulations, you passed the ngram check!\")\n",
    "    \n",
    "  \n",
    "test_ngram_lm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EjKT7pwJE6WY"
   },
   "source": [
    "### **Q1.2:** Generate text from n-gram language model (10pts)\n",
    "\n",
    "Complete the following *generate_text* function based on these input/output requirements:\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the lm object, a dictionary you return from  the **train_ngram_lm** function\n",
    "+ **vocab**: vocab is a list of unique word types in the training set computed already computed for you during data loading.\n",
    "+ **context**: the input context string that you want to condition your language model on, should be a space-separated string of tokens\n",
    "+ **order**: order of your language model (i.e., \"n\" in the n-gram model)\n",
    "+ **num_tok**: number of tokens to be generated following the input context\n",
    "\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ generated text, should be a space-separated string\n",
    "    \n",
    "*Hint:*\n",
    "\n",
    "After getting the next-word distribution given history, try using **[numpy.random.choice](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.choice.html)** to sample the next word from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wy__Q6YJk0h8"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jx0BFoF1E5dF"
   },
   "outputs": [],
   "source": [
    "# generate text\n",
    "def generate_text(lm, vocab, context=\"he is the\", order=3, num_tok=25):\n",
    "    \n",
    "    # The goal is to generate new words following the context\n",
    "    # If context has more tokens than the order of lm, \n",
    "    # generate text that follows the last (order-1) tokens of the context\n",
    "    # and store it in the variable `history`\n",
    "    order -= 1\n",
    "    history = context.split()[-order:]\n",
    "    # `out` is the list of tokens of context\n",
    "    # you need to append the generated tokens to this list\n",
    "    out = context.split()    \n",
    "    for i in range(num_tok):\n",
    "      key = ' '.join(history)\n",
    "      if key not in lm.keys():\n",
    "        next_word = np.random.choice(vocab)\n",
    "      else:\n",
    "        next_word = np.random.choice(list(lm[key].keys()))\n",
    "      context = ' '.join([context, next_word])\n",
    "      history = context.split()[-order:]\n",
    "      out = context.split()\n",
    "\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2qPPhLK3HF5L"
   },
   "source": [
    "Now try to generate some texts! Read the texts generated by ngram language model with different orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "BSlNevanHIlM",
    "outputId": "335eb558-2ab8-4ac5-b1a2-476b64fb69b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the vocalization carcetti moniteur natured gatwick palabras highlander usury shepherd andorian outdoors open ritualised 1142 1166 poker evaporation montenegrins dahl camden kramer soledad loftleidir suspicious hoot'"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 1\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ibmvkwl9HMzd",
    "outputId": "268e29d5-8321-425d-917f-ec9fec60a3c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the scoring 7 rating due on 70 mph . philippe and enlarged portion by ellen intensified . raw and bourdais in 1856 at dalmeny and wendy'"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 2\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BQNB3FqKHibm",
    "outputId": "831c6ff8-f2aa-4477-85e3-ff1962ebce30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the simple looks and <unk> seaplane which attempted to position his four star review for president he says effectively changed fashion in ireland was largely described'"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 3\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eSO4l5z7HjGU",
    "outputId": "612abfbf-ab78-4486-bd7a-8402b0564b9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the same and their philosophical views are similar : neither \" pays much attention to death , because that \\'s basic to all of the amino'"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 4\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLovkCIrHy0H"
   },
   "source": [
    "### Q1.3 : Complete *compute_perplexity* (15pts)\n",
    "Now let's evaluate the models quantitively using the intrinsic metric **perplexity**. \n",
    "\n",
    "Recall perplexity is the inverse probability of the test text\n",
    "$$\\text{ppl}(w_1, \\dots, w_n) = p(w_1, \\dots, w_n)^{-\\frac{1}{N}}$$\n",
    "\n",
    "For an n-gram model, perplexity is computed by\n",
    "$$\\text{ppl}(w_1, \\dots, w_n) = (\\prod_i p(w_{i+n}|w_i^{i+n-1})^{-\\frac{1}{N}}$$\n",
    "\n",
    "To get rid of numerical issue, we usually compute through:\n",
    "$$\\text{ppl}(w_1, \\dots, w_n) = \\exp(-\\frac{1}{N}\\sum_i \\log p(w_{i+n}|w_i^{i+n-1}))$$\n",
    "\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the language model you trained, still the object you returned from the **train_ngram_lm** function\n",
    "+ **data**: test data\n",
    "+ **vocab**: vocab\n",
    "+ **order**: order of the lm\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ the perplexity of test data\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "+ If the history is not in the **lm** object, back-off to (n-1) order history to check if it is in **lm**. If no history can be found, just use `1/|V|` where `|V|` is the size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKi4AczgHj1t"
   },
   "outputs": [],
   "source": [
    "def compute_perplexity(lm, data, vocab, order=3):\n",
    "    \n",
    "    # pad according to order\n",
    "    order -= 1\n",
    "    data = ['<S>'] * order + data\n",
    "    ppl = 0\n",
    "    vocab = vocab + ['<S>']\n",
    "    for i in range(len(data) - order):\n",
    "      h, w = ' '.join(data[i: i+order]), data[i+order]\n",
    "      \"\"\"\n",
    "      IMPLEMENT ME!\n",
    "      # if h not in lm, back-off to n-1 gram and look up again\n",
    "\n",
    "      \"\"\"\n",
    "      \n",
    "      while (h not in lm.keys()) or (lm[h][w] == 0):\n",
    "        if len(h.split()) == 1:\n",
    "          h = ''\n",
    "        else:\n",
    "          h = ' '.join(h.split()[1:])\n",
    "      \n",
    "      ppl += np.log(lm[h][w])\n",
    "    \n",
    "    ppl = np.exp( (-1/(len(data) - order)) * ppl )\n",
    "\n",
    "    return ppl    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hVpItwZhI6ac"
   },
   "source": [
    "Let's evaluate the language model with different orders. You should see a decrease in perplexity as the order increases. As a reference, the perplexity of the unigram, bigram, trigram, 4-gram language model should be around 795, 203, 141, 130 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "SpN70HA2H9C-",
    "outputId": "fe06da94-1ee4-48b3-8250-c19035dde1ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order 1 ppl 794.5377104541699\n",
      "order 2 ppl 203.26492650337096\n",
      "order 3 ppl 141.22462003506539\n",
      "order 4 ppl 129.6418792167028\n"
     ]
    }
   ],
   "source": [
    "for o in [1, 2, 3, 4]:\n",
    "    lm = train_ngram_lm(data['train'], order=o)\n",
    "    print('order {} ppl {}'.format(o, compute_perplexity(lm, data['test'], vocab, order=o)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9tRCKZ5BJJOV"
   },
   "source": [
    "## Q2. Neural language models (60pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jlXIjm6WZBE"
   },
   "source": [
    "In this part of the homework, we'll be using PyTorch to play around with neural language models. First, a quick warm up by implementing backpropagation within a *scalar* neural network. Then, you'll implement a neural language model using PyTorch's built-in modules.\n",
    "\n",
    "Firstly, run the cell below to import pytorch and set up the gradient checking functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOhQHHAPV6LD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# checks equality between your gradients and those from autograd\n",
    "def gradient_check(params, your_gradient):\n",
    "    all_good = True\n",
    "    for key in params.keys():\n",
    "        if params[key].grad.size() != your_gradient[key].size():\n",
    "            print('GRADIENT ERROR for parameter %s, SIZE ERROR\\nyour size: %s\\nactual size: %s\\n'\\\n",
    "                % (key, your_gradient[key].size(), \n",
    "                   params[key].grad.size()))\n",
    "            all_good = False\n",
    "        elif not torch.allclose(params[key].grad, your_gradient[key], atol=1e-6):\n",
    "            print('GRADIENT ERROR for parameter %s, VALUE ERROR\\nyours: %s\\nactual: %s\\n'\\\n",
    "                % (key, your_gradient[key].detach(), \n",
    "                   params[key].grad))\n",
    "            all_good = False\n",
    "            \n",
    "    return all_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kt0UNgpWuJ6"
   },
   "source": [
    "### Q2.1 Warm up with single neuron (5 pts)\n",
    "The below cell trains a network with scalars (i.e., single neurons) in each layer on a small dataset of ten examples. We saw a similar architecture in the class notes. All you have to do is translate the partial derivatives we computed into code. The network is defined as:\n",
    "\n",
    "<center>$\\text{h} = \\tanh(w_1 \\cdot \\text{input})$</center>\n",
    "\n",
    "<center>$\\text{pred} = \\tanh(w_2 \\cdot \\text{h})$</center>\n",
    "\n",
    "<center>$\\text{L} = 0.5 \\cdot (\\text{target} - \\text{pred})^2$</center>\n",
    "\n",
    "If you run the cell below, you should see \"GRADIENT ERRORS\". Once you implement the partial derivatives $\\frac{\\partial{L}}{\\partial{w_1}}$ and $\\frac{\\partial{L}}{\\partial{w_2}}$ correctly, you will instead see a \"SUCCESS\" message. **Do NOT modify any code outside of the block marked \"IMPLEMENT BACKPROP HERE\"!**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HfLgwTNYWGt3",
    "outputId": "b85d7f42-89ef-48a8-a7bf-07634573f7c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS! you passed the gradient check.\n"
     ]
    }
   ],
   "source": [
    "# initialize model parameters\n",
    "params = {}\n",
    "params['w1'] = torch.randn(1, 1, requires_grad=True) # input > hidden with scalar weight w1\n",
    "params['w2'] = torch.randn(1, 1, requires_grad=True) # hidden > output with scalar weight w2\n",
    "\n",
    "# set up some training data\n",
    "inputs = torch.randn(20, 1)\n",
    "targets = inputs / 2\n",
    "\n",
    "# training loop\n",
    "all_good = True\n",
    "for i in range(len(inputs)):\n",
    "    \n",
    "    ## forward prop, then compute loss.\n",
    "    a = params['w1'] * inputs[i] # intermediate variable, following lecture notes\n",
    "    hidden = torch.tanh(a)\n",
    "    b = params['w2'] * hidden\n",
    "    pred = torch.tanh(b)\n",
    "    loss = 0.5 * (targets[i] - pred) ** 2 # compute square loss\n",
    "    loss.backward() # runs autograd\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    # TODO: IMPLEMENT BACKPROP HERE\n",
    "    # DO NOT MODIFY ANY CODE OUTSIDE OF THIS BLOCK!!!!\n",
    "    your_gradient = {}\n",
    "    your_gradient['w1'] = torch.zeros(params['w1'].size()) # implement dL/dw1\n",
    "    your_gradient['w2'] = torch.zeros(params['w2'].size()) # implement dL/dw2\n",
    "    \n",
    "    # IMEPLEMENT ME!\n",
    "    dh_w1 = (1 - torch.tanh(a) ** 2) * inputs[i]\n",
    "    dpred_w1 = (1 - torch.tanh(b) ** 2) * params['w2'] * dh_w1\n",
    "    your_gradient['w1'] = -1 * (targets[i] - pred) * dpred_w1\n",
    "    \n",
    "    dpred_w2 = (1 - torch.tanh(b) ** 2) * hidden\n",
    "    your_gradient['w2'] = -1 * (targets[i] - pred) * dpred_w2\n",
    "    # END \n",
    "    ####################\n",
    "    \n",
    "    if not gradient_check(params, your_gradient):\n",
    "        all_good = False\n",
    "        break\n",
    "    \n",
    "    # zero gradients after each training example\n",
    "    params['w1'].grad.zero_()\n",
    "    params['w2'].grad.zero_() \n",
    "    \n",
    "if all_good:\n",
    "    print('SUCCESS! you passed the gradient check.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5bzxVSAOPUn"
   },
   "source": [
    "### Q2.2 RNN language model (10 pts)\n",
    "\n",
    "For this part of the homework, we will use **PyTorch** to build our model. The below cell preprocesses the raw text so you can load it directly. The input to your model is a *minibatch* of sequences which takes the form of a  $N \\times L$ matrix  where $N$ is the batch size and $L$ is the maximum sequence length. For each minibatch, your models should produce an $N \\times L \\times V$ tensor where $V$ is the size of the vocabulary. This tensor stores the predicted probability distribution of the next word for every position of every sequence in the batch. Note that each batch is padded to dimensionality $L=40$ using the special padding token <*pad>*; similarly, each sequence begins with the <*bos>* token and ends with the <*eos>* token. Please look at the PyTorch RNN documentation if you're having problems getting started: https://pytorch.org/docs/stable/nn.html#rnn\n",
    "\n",
    "Firstly run the cell below to download the data. **If you see \"device: cpu\", please change your Colab runtime to the GPU backend by going to \"Runtime > Change runtime type > Hardware accelerator > GPU\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "xtOZWDtTSCAG",
    "outputId": "fbf317f6-8b4a-44f1-f9e7-03e46a54bb16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda:0\n",
      "Wikitext data downloaded!\n",
      "There are 28654 words in vocabulary\n",
      "Word id 0 stands for '<pad>'\n",
      "Word id 1 stands for '<unk>'\n",
      "Word id 2 stands for '<bos>'\n",
      "Word id 3 stands for '<eos>'\n",
      "Word id 4 stands for 'the'\n",
      "Word id 5 stands for ','\n",
      "Word id 6 stands for '.'\n",
      "Word id 7 stands for 'of'\n",
      "...\n",
      "tensor(1622368, device='cuda:0')\n",
      "Set up finished\n"
     ]
    }
   ],
   "source": [
    "import torch, pickle, os, sys, random, time\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device: ', device)\n",
    "\n",
    "# Download id2word\n",
    "f_wikitext = drive.CreateFile({'id': '1fBS7PyEOeQMuH5Ea1_hnEjU3PmFE7ZZc'})\n",
    "f_wikitext.GetContentFile('./wikitext.pkl') \n",
    "with open('./wikitext.pkl', 'rb') as f_in:\n",
    "  wikitext = pickle.load(f_in)\n",
    "  \n",
    "wikitext['train'] = torch.LongTensor(wikitext['train']).to(device)\n",
    "wikitext['dev'] = torch.LongTensor(wikitext['valid']).to(device)\n",
    "wikitext['test'] = torch.LongTensor(wikitext['test']).to(device)\n",
    "idx_to_word = wikitext['id2word']\n",
    "word_to_idx = {idx_to_word[k]: k for k in idx_to_word}\n",
    "\n",
    "\n",
    "print(\"Wikitext data downloaded!\")\n",
    "# Demonstrate id2word\n",
    "print('There are ' + str(len(idx_to_word)) + ' words in vocabulary')\n",
    "for id in range(8):\n",
    "  print('Word id ' + str(id) + \" stands for '\" + str(idx_to_word[id]) + \"\\'\")\n",
    "print('...')\n",
    "print((wikitext['train'] > 0).sum())\n",
    "    \n",
    "print('Set up finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_3C-13G_m2Q"
   },
   "source": [
    "The following cell contains code for computing perplexity and training the neural language model. Run the cell, and please make sure you (at least roughly) understand what is happening, but **do not modify any part of it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "onfjIrblON0g"
   },
   "outputs": [],
   "source": [
    "# function to evaluate LM perplexity on some input data, DO NOT MODIFY\n",
    "def compute_perplexity(dataset, net, bsz=64):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "    num_examples, seq_len = dataset.size()\n",
    "    \n",
    "    # we'll still use batches b/c we can't fit the whole\n",
    "    # validation set into GPU memory\n",
    "    batches = [(start, start + bsz) for start in\\\n",
    "               range(0, num_examples, bsz)]\n",
    "    \n",
    "    total_unmasked_tokens = 0. # count how many unpadded tokens there are\n",
    "    nll = 0.\n",
    "    for b_idx, (start, end) in enumerate(batches):\n",
    "            \n",
    "        batch = dataset[start:end]\n",
    "        ut = torch.nonzero(batch).size(0)\n",
    "        preds = net(batch)\n",
    "        targets = batch[:, 1:].contiguous().view(-1)\n",
    "        preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
    "        loss = criterion(preds, targets)\n",
    "        nll += loss.detach()\n",
    "        total_unmasked_tokens += ut\n",
    "\n",
    "    perplexity = torch.exp(nll / total_unmasked_tokens).cpu()\n",
    "    return perplexity.data\n",
    "    \n",
    "\n",
    "# training loop for language models, DO NOT MODIFY!\n",
    "def train_lm(dataset, params, net):\n",
    "    \n",
    "    # since the first index corresponds to the PAD token, we just ignore it\n",
    "    # when computing the loss\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    num_examples, seq_len = dataset.size()    \n",
    "    batches = [(start, start + params['batch_size']) for start in\\\n",
    "               range(0, num_examples, params['batch_size'])]\n",
    "    \n",
    "    for epoch in range(params['epochs']):\n",
    "        ep_loss = 0.\n",
    "        start_time = time.time()\n",
    "        random.shuffle(batches)\n",
    "        net.train()\n",
    "        # for each batch, calculate loss and optimize model parameters            \n",
    "        for b_idx, (start, end) in enumerate(batches):\n",
    "                        \n",
    "            batch = dataset[start:end]\n",
    "            preds = net(batch)\n",
    "\n",
    "            preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
    "            # q1.1: explain the below line!\n",
    "            targets = batch[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(preds, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            ep_loss += loss\n",
    "        \n",
    "        net.eval()\n",
    "        print('epoch: %d, loss: %0.2f, time: %0.2f sec, dev perplexity: %0.2f' %\\\n",
    "              (epoch, ep_loss, time.time()-start_time, compute_perplexity(wikitext['dev'], net)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVwFC8cHLWye"
   },
   "source": [
    "Now implement the following class, which defines a recurrent neural language model, by filling in the __forward__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IBC5pTTgIBVH"
   },
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.vocab_size = params['vocab_size']\n",
    "        self.d_emb = params['d_emb']\n",
    "        self.d_hid = params['d_hid']\n",
    "        self.n_layer = 1\n",
    "        self.batch_size = params['batch_size']\n",
    "        \n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.d_emb)\n",
    "        self.rnn = nn.RNN(self.d_emb, self.d_hid, self.n_layer, batch_first=True)\n",
    "        self.decoder = nn.Linear(self.d_hid, self.vocab_size)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "            IMPLEMENT ME!\n",
    "            Encode the data using the embedding layer you initialized.\n",
    "            Pass the encoded data and hidden states to your RNN.\n",
    "            Return unnormalized logits for each token's prediction.\n",
    "            \n",
    "            Why just logits? Check the document of torch.nn.CrossEntropyLoss,\n",
    "            since it combines nn.LogSoftmax() and nn.NLLLoss(), \n",
    "            you don't need to explicitly use the softmax function!\n",
    "        \"\"\"\n",
    "        batch_size, seq_len= batch.shape\n",
    "        hidden = (torch.zeros(self.n_layer, batch_size, self.d_hid).to(device))\n",
    "        embedded_input = self.encoder(batch)\n",
    "        output, hidden = self.rnn(embedded_input)\n",
    "        decoded_output = self.decoder(output)\n",
    "        # scores = torch.nn.CrossEntropyLoss(decoded_output)\n",
    "        return decoded_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfzBU3SCWIso"
   },
   "source": [
    "Run the following cell to test that your implementation is at least returning tensors of the proper dimensionality. Note that this is just a sanity check to help you develop. Your RNNLM might still be implemented incorrectly even if it passes. You will have to obtain a reasonable perplexity after training on WikiText to be certain that you've done it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wv3NEVi7AaCE",
    "outputId": "3c1d315d-c7a4-402c-c7b1-d47448a6adeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations, you passed the RNNLM test!\n"
     ]
    }
   ],
   "source": [
    "def test_RNNLM():\n",
    "    test_batch = torch.LongTensor(5, 4).random_(0, 10).to(device)\n",
    "    params = {}\n",
    "    params['vocab_size'] = len(idx_to_word)\n",
    "    params['d_emb'] = 8\n",
    "    params['d_hid'] = 8\n",
    "    params['batch_size'] = 5\n",
    "    testnet = RNNLM(params)\n",
    "    testnet.to(device)\n",
    "    test_output = testnet(test_batch)\n",
    "    assert test_output.shape[0] == params['batch_size'], \"size of dimension 0 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['batch_size'], test_output.shape[0])\n",
    "    assert test_output.shape[1] == test_batch.shape[1], \"size of dimension 1 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (test_batch.shape[1], test_output.shape[1])\n",
    "    assert test_output.shape[2] == params['vocab_size'], \"size of dimension 2 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['vocab_size'], test_output.shape[2])\n",
    "    print(\"Congratulations, you passed the RNNLM test!\")\n",
    "test_RNNLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7li14NHkLimS"
   },
   "source": [
    "Once you pass the above test, train your RNNLM model on WikiText by running the below cell. It should take a couple minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "tGvKkmqMET1w",
    "outputId": "1002a749-1650-4735-de76-02bbcc1962ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 6410.59, time: 97.68 sec, dev perplexity: 185.72\n",
      "epoch: 1, loss: 5667.06, time: 97.68 sec, dev perplexity: 156.67\n",
      "epoch: 2, loss: 5319.58, time: 97.84 sec, dev perplexity: 148.48\n",
      "epoch: 3, loss: 5065.57, time: 97.80 sec, dev perplexity: 147.22\n",
      "epoch: 4, loss: 4858.05, time: 97.94 sec, dev perplexity: 147.93\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THESE HYPERPARAMETERS, WE WILL CHECK!\n",
    "params = {}\n",
    "params['vocab_size'] = len(idx_to_word)\n",
    "params['d_emb'] = 512\n",
    "params['d_hid'] = 256\n",
    "params['batch_size'] = 64\n",
    "params['epochs'] = 5\n",
    "params['learning_rate'] = 0.001\n",
    "\n",
    "RNNnet = RNNLM(params)\n",
    "RNNnet.to(device)\n",
    "train_lm(wikitext['train'], params, RNNnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jwNvc39bLnqY"
   },
   "source": [
    "After training is finished, run the below cell to get the perplexity on the test set. If you did it right, your perplexity should be around 135-140."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xD6NYw62St2J",
    "outputId": "b4debf61-e82b-420d-ceaa-396c6b315246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 137.40\n"
     ]
    }
   ],
   "source": [
    "RNNnet.eval() # we're no longer training the network\n",
    "print('%s perplexity: %0.2f' % ('test', compute_perplexity(wikitext['test'], RNNnet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DqYr-aIXUsU-"
   },
   "source": [
    "### Q2.3 Explain the code (5 pts)\n",
    "\n",
    "These lines in the provided **compute_perplexity** function: \n",
    "\n",
    "```\n",
    "            preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
    "            targets = batch[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(preds, targets)\n",
    "```\n",
    "\n",
    "Look at the documentation of the CrossEntropyLoss criterion [here](https://pytorch.org/docs/stable/nn.html#crossentropyloss). Then, explain what these lines accomplish by answering the following question:\n",
    "\n",
    "*   What does the indexing in front of *batch* accomplish in the second line? Why can't we just use the *batch* variable as the target? (5 pts)\n",
    "\n",
    "  * The indexing in front of *batch* removes the *\\<bos\\>* token which pads the start of a sequence. This is because, the *CrossEntropyLoss* criterion as per its documentation calculates the loss based on the class of the target, where the *\\<bos\\>* token bring in noise to the error computed. Also the documentation expects the target class indexes in the range of [0,C-1]. Thus using the batch variable directly as the target will obtain incorrect results, as the probability distribution of the words will be shifted with the presence of the *\\<bos\\>* token that affects the probability of the successive word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKu_fZJ6VUL8"
   },
   "source": [
    "### Q2.4 Neural Language Model with attention (25 pts)\n",
    "\n",
    "Only start working on this after you've correctly implemented the RNNLM in the previous problem, as you'll want to copy over some code here. \n",
    "Complete the foward function of both the **ATTNLM** and **Attention** modules by following the instructions in the comment block. **Each epoch may take 3-5 minutes to run, so start early!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ja339lSzVajG"
   },
   "outputs": [],
   "source": [
    "# An RNN language model with attention, you implement this!\n",
    "class ATTNLM(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(ATTNLM, self).__init__()\n",
    "        \n",
    "        self.vocab_size = params['vocab_size']\n",
    "        self.d_emb = params['d_emb']\n",
    "        self.d_hid = params['d_hid']\n",
    "        self.n_layer = 1\n",
    "        self.btz = params['batch_size']\n",
    "        \n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.d_emb)\n",
    "        self.attn = Attention(self.d_hid)\n",
    "        self.rnn = nn.RNN(self.d_emb, self.d_hid, self.n_layer, batch_first=True)\n",
    "        # the combined_W maps the combined hidden states and context vectors to d_hid \n",
    "        self.combined_W = nn.Linear(self.d_hid * 2, self.d_hid)\n",
    "        self.decoder = nn.Linear(self.d_hid, self.vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, batch, return_attn_weights=False):\n",
    "        \n",
    "        \"\"\"\n",
    "            IMPLEMENT ME!\n",
    "            Copy your implementation of RNNLM, make sure it passes the RNNLM check\n",
    "            In addition to that, you need to add the following 3 things\n",
    "            1. pass rnn output to attention module, get context vectors and attention weights\n",
    "            2. concatenate the context vec and rnn output, pass the combined\n",
    "               vector to the layer dealing with the combined vectors (self.combined_W)\n",
    "            3. if return_attn_weights, instead of return the [N, L, V]\n",
    "               matrix, return the attention weight matrix\n",
    "               of dimension [N, L, L] which returned from the forrward function of Attnetion module\n",
    "        \"\"\"\n",
    "        batch_size, seq_len= batch.shape\n",
    "        \n",
    "        hidden = torch.zeros(self.n_layer, batch_size, self.d_hid).to(device)\n",
    "\n",
    "        embedded_input = self.encoder(batch)\n",
    "        output, hidden = self.rnn(embedded_input)\n",
    "        context_vec, attn_wgt = self.attn(output)\n",
    "\n",
    "        if not return_attn_weights:\n",
    "          comb_inp = torch.cat((context_vec, output), 2)\n",
    "          comb_out = self.combined_W(comb_inp)\n",
    "          y_pred = self.decoder(comb_out)\n",
    "        else:\n",
    "          y_pred = attn_wgt\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_hidden):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear_w1 = nn.Linear(d_hidden, d_hidden)\n",
    "        self.linear_w2 = nn.Linear(d_hidden, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "      \n",
    "        \"\"\"\n",
    "            IMPLEMENT ME!\n",
    "            For each time step t\n",
    "                1. Obtain attention scores for step 0 to (t-1)\n",
    "                   This should be a dot product between current hidden state (x[:,t:t+1,:])\n",
    "                   and all previous states x[:, :t, :]. While t=0, since there is not\n",
    "                   previous context, the context vector and attention weights should be of zeros.\n",
    "                   You might find torch.bmm useful for computing over the whole batch.\n",
    "                2. Turn the scores you get for 0 to (t-1) steps to a distribution.\n",
    "                   You might find F.softmax to be helpful.\n",
    "                3. Obtain the sum of hidden states weighted by the attention distribution\n",
    "            Concat the context vector you get in step 3. to a matrix.\n",
    "            \n",
    "            Also remember to store the attention weights, the attention matrix \n",
    "            for each training instance should be a lower triangular matrix. Specifically,\n",
    "            each row, element 0 to t-1 should sum to 1, the rest should be padded with 0.\n",
    "            e.g. \n",
    "            [ [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "              [1.0000, 0.0000, 0.0000, 0.0000],\n",
    "              [0.4246, 0.5754, 0.0000, 0.0000],\n",
    "              [0.2798, 0.3792, 0.3409, 0.0000] ]\n",
    "            \n",
    "            Return the context vector matrix and the attention weight matrix\n",
    "            \n",
    "        \"\"\"\n",
    "        batch_size, batch_seq_len, vocab_len = x.shape\n",
    "        \n",
    "        attn_weights = torch.zeros(batch_size, batch_seq_len, batch_seq_len)\n",
    "        context_vectors = torch.zeros(batch_size, batch_seq_len, vocab_len)\n",
    "       \n",
    "        attn_scores = torch.bmm(x.clone(), x.clone().permute(0,2,1))\n",
    "        attn_scores = torch.tril(attn_scores, diagonal=-1)\n",
    "        attn_scores[attn_scores == 0] = -1e6  # setting v small value\n",
    "        attn_scores = F.softmax(attn_scores, dim=2)\n",
    "        attn_weights = torch.tril(attn_scores, diagonal=-1)\n",
    "\n",
    "        context_vectors = torch.bmm(attn_weights.clone(), x.clone())\n",
    "        \n",
    "        return context_vectors, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqyjOwGz8Y6b"
   },
   "source": [
    "Run the following cell to sanity check your implementation; do not continue until you pass all of the tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_0-MHHC2KYv8",
    "outputId": "dd4d4ffb-4b64-4476-ecd4-f9b5b30158d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations, you passed the ATTNLM test!\n"
     ]
    }
   ],
   "source": [
    "def test_ATTNLM():\n",
    "    test_batch = torch.LongTensor(5, 4).random_(0, 10).to(device)\n",
    "    params = {}\n",
    "    params['vocab_size'] = len(idx_to_word)\n",
    "    params['d_emb'] = 8\n",
    "    params['d_hid'] = 8\n",
    "    params['batch_size'] = 5\n",
    "    testnet = ATTNLM(params)\n",
    "    testnet.to(device)\n",
    "    test_output = testnet(test_batch)\n",
    "    assert test_output.shape[0] == params['batch_size'], \"size of dimension 0 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['batch_size'], test_output.shape[0])\n",
    "    assert test_output.shape[1] == test_batch.shape[1], \"size of dimension 1 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (test_batch.shape[1], test_output.shape[1])\n",
    "    assert test_output.shape[2] == params['vocab_size'], \"size of dimension 2 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['vocab_size'], test_output.shape[2])\n",
    "    testnet = ATTNLM(params)\n",
    "    testnet.to(device)\n",
    "    test_output = testnet(test_batch, return_attn_weights=True)\n",
    "    assert test_output.shape[0] == params['batch_size'], \"size of dimension 0 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['batch_size'], test_output.shape[0])\n",
    "    assert test_output.shape[1] == test_batch.shape[1], \"size of dimension 1 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (test_batch.shape[1], test_output.shape[1])\n",
    "    assert test_output.shape[2] == test_batch.shape[1], \"size of dimension 2 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (test_batch.shape[1], test_output.shape[2])\n",
    "    prob_dist = torch.sum(test_output, dim=2)[:, 1:]\n",
    "    assert all([x > 0.99 and x < 1.01 for x in prob_dist.reshape(-1)]), \"attention weights not properly normalized, got {}\".format(prob_dist)\n",
    "    print(\"Congratulations, you passed the ATTNLM test!\")\n",
    "\n",
    "test_ATTNLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vRxeoJW3Ltzk"
   },
   "source": [
    "Now, train your ATTNLM model on WikiText by running the below cell. If the perplexity on dev set is `nan` or `inf`, it is likely the model has corrupted due to gradient exploding/vanishing or other numerical instability issue, stop this cell and run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "81LKL_7pKYAC",
    "outputId": "d1f10b9f-24f3-48d5-e69d-2e4843b0ec08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 6532.14, time: 100.19 sec, dev perplexity: 206.09\n",
      "epoch: 1, loss: 5889.90, time: 100.27 sec, dev perplexity: 174.33\n",
      "epoch: 2, loss: 5599.99, time: 100.21 sec, dev perplexity: 161.79\n",
      "epoch: 3, loss: 5365.44, time: 100.29 sec, dev perplexity: 153.47\n",
      "epoch: 4, loss: 5158.72, time: 100.43 sec, dev perplexity: 153.71\n",
      "epoch: 5, loss: 4973.99, time: 100.33 sec, dev perplexity: 153.40\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THESE HYPERPARAMETERS, WE WILL CHECK!\n",
    "params = {}\n",
    "params['vocab_size'] = len(idx_to_word)\n",
    "params['d_emb'] = 512\n",
    "params['d_hid'] = 256\n",
    "params['n_layer'] = 1\n",
    "params['batch_size'] = 64\n",
    "params['epochs'] = 6\n",
    "params['learning_rate'] = 0.0005\n",
    "\n",
    "ATTNnet = ATTNLM(params)\n",
    "ATTNnet.cuda()\n",
    "train_lm(wikitext['train'], params, ATTNnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENa03ZlJLyFF"
   },
   "source": [
    "Finally, compute the perplexity on the test set. If you implemented it correctly, you should get a perplexity of around 145-150. Due to random effects, it is possible to get perplexity slightly lower than 145. Make sure you didn't add any additional nonlinearity operation which can lead to lower perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LoOvc6quW0Ef",
    "outputId": "c44234d1-8dc3-4ca3-9b72-89b094e18bf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 143.36\n"
     ]
    }
   ],
   "source": [
    "ATTNnet.eval() # we're no longer training the network\n",
    "print('%s perplexity: %0.2f' % ('test', compute_perplexity(wikitext['test'], ATTNnet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eSJQCTtCgcb"
   },
   "source": [
    "### Q2.5 Explain the results (5 pts)\n",
    "\n",
    "If you implemented this correctly, you should notice a similar (or slightly higher) perplexity for the ATTNLM than for RNNLM. Considering attention is supposed to be a net improvement for RNN-type models, give two possible reasons why the ATTNLM's perplexity isn't significantly lower.\n",
    "\n",
    "  * Attention based Language Models usually require a significantly bigger amount of training data set to better predict the next token based on the context of the previous words. Compared to an RNNLM which doesn't use attention, it considers the entire probability distribution set and in case of a small input data set, its likely that the perplexity of the ATTNLM isn't significantly lower than the RNNLM.\n",
    "  * In the provided case, the RNN language model might have better parameters as compared to the ATTNLM language model. Such as the learning rate for the RNNLM greater than that of the ATTNLM, which implies that the ATTNLM model may achieve better optimization with smaller step sizes while optimizing the loss, but it takes more time in finding the optimal solution compared to the RNN. Hence comparison of the two models after the same number of epochs might not provide us with a good judgement as the ATTNLM model might have a better model in a futher more epochs later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCyBN6KtEpLy"
   },
   "source": [
    "### Q2.6 Generate text from the neural LMs (5 pts)\n",
    "Run the below cell to generate some text from your RNNLM and ATTNLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fkzzhEYCCY5A",
    "outputId": "c285f981-5857-43d7-9ed7-302af32c0766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn completion:  he is the four @-@ relief hull .\n"
     ]
    }
   ],
   "source": [
    "def sample_from_lm(net, context, max_words=50):\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for i in range(max_words):\n",
    "            data = torch.LongTensor([context]).to(device)\n",
    "            decoded = net(data)\n",
    "            decoded = decoded[0, -1].exp().cpu()\n",
    "            w_i = torch.multinomial(decoded, 1)[0].item()\n",
    "            if w_i in [1, 2, 3]:\n",
    "                continue\n",
    "            context.append(w_i)\n",
    "\n",
    "        return context\n",
    "\n",
    "word_to_idx = dict((v,k) for (k,v) in idx_to_word.items())\n",
    "context = [word_to_idx[w] for w in 'he is the '.split()]\n",
    "\n",
    "rnn_completion = sample_from_lm(RNNnet, context)\n",
    "print('rnn completion: ', ' '.join([idx_to_word[w] for w in rnn_completion]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QYg9ki0dYoyi",
    "outputId": "e4b9bc13-c2cd-4cc4-f3e6-508248301637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention rnn completion:  he is the second national football league cup .\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = dict((v,k) for (k,v) in idx_to_word.items())\n",
    "context = [word_to_idx[w] for w in 'he is the '.split()]\n",
    "\n",
    "rnn_completion = sample_from_lm(ATTNnet, context)\n",
    "print('attention rnn completion: ', ' '.join([idx_to_word[w] for w in rnn_completion]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KU63vLoNE76l"
   },
   "source": [
    "Do you notice any differences in coherence or grammaticality compared to the n-gram models? What about any differences between the RNNLM and the ATTNLM? If you observed any distinct differences, explain why you think they exist; if not, explain why all of the outputs appear to be of similar quality.\n",
    "  * The sentences generated by the n-gram models were highly inconsistent with their grammatical sense as well as the syntaxes were completely off. Though some runs did show better sentence formulations, but it lasted only for a couple of words beyond which it becomes non sensible. Whereas the RNNLM and the ATTNLM models show better consistency in grammatical senses, though as can be seen it too sometimes throws words out of context which ruin the flow of the sentence. The sentences produced by the RNNLM and the ATTNLM as can be seen, show more apt sentences in case of the ATTNLM as compared to the RNNLM. On multiple runs though we see, that the sentences generated were of highly similar quality. This is due to the small size of the training data set which incurs a higher relation to the probability distribution of the words associated with its context.The ATTNLM model should show distinct differences compared to the RNNLM since the attention models are better capable of understanding the meaning of the entire sentence as compared to RNN language models which tries to consider every word possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xBGJPkitfizQ"
   },
   "source": [
    "### Q2.7 Interpreting attention (5 pts)\n",
    "Finally, let's visualize some attention heatmaps by running the below two cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "jXykMRedFfE2",
    "outputId": "496592b1-4183-4d3e-e87d-64504c302318"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASAAAAEYCAYAAAD8qitAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_attn_heatmap(sent):\n",
    "  \n",
    "    sent_in_id = [word_to_idx[w] for w in sent.split()]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data = torch.LongTensor([sent_in_id]).to(device)\n",
    "        weights = ATTNnet(data, return_attn_weights=True)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sent_sp = sent.split()\n",
    "    ax.set_xticks(np.arange(len(sent_sp)))\n",
    "    ax.set_yticks(np.arange(len(sent_sp)))\n",
    "    ax.set_xticklabels(sent_sp)\n",
    "    ax.set_yticklabels(sent_sp)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode=\"anchor\")\n",
    "\n",
    "    plt.imshow(weights[0, :].cpu())\n",
    "\n",
    "sent = \"top warning signs earth is warming , according to experts\"\n",
    "plot_attn_heatmap(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "colab_type": "code",
    "id": "szDBrOdpFflZ",
    "outputId": "e224af90-6c1c-41ce-8bab-3f682463045d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARUAAAERCAYAAABVfzP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO2dd7gV1fW/3w+IgCAiVrAhQVQsoIIF\nG7GLBgtWbGiUaKIYFY2Jfv0lJrZoolHT0BhJRGPEEmMJ9t6NgF1jNLEkxtgxiIjr98faR4bDuX3O\nOXO5632e+9xpe/Y658x89tpr71kjMyMIgiAvOtXbgCAIFi1CVIIgyJUQlSAIciVEJQiCXAlRCYIg\nV0JUgiDIlcXqbUA1WFxdrRs96m1GECzSfMz7/zWz5cq3L5Ki0o0ebKJt621GECzS3GFT/1Fpe3R/\ngiDIlRCVIAhyJUQlCIJcCVEJgiBXChWoldQfuMnM1k3rE4GewHvAkcDnwHNmtl+9bAyCoHEKJSqN\ncDKwupnNkdS70gGSxgPjAbqxRC1tC4IgQ3vp/swEpkg6EPdWFsLMJpnZMDMb1oWutbUuCIIvKZqo\nfM6CNnVL/3cBfg5sCDwuqb14WEHQ4SiaqLwNLC9pGUldgV1xG1cxs7uB7wBL4XGWIAgKSKFafDOb\nK+l04DHgTeAFoDNwhaSlAAEXmtkHdTQzCIJGKJSoAJjZhcCF9bYjCILWUbTuTxAE7ZwQlSAIcqVw\n3Z88GLT+/5g2bXqry+/Yb2iO1gRBxyI8lSAIciVEJQiCXAlRCYIgV2oiKpKOlHRwWh4nqV9m36WS\nBtfCjiAIqk9NArVm9qvM6jjgGeCttO/wWtgQBEFtqIqoJK9kImD4w4CvALOA14Bh+MOBs4HNgFuB\niWb2hKQdgB8AXVOZQ81slqSzgdH4s0G3mdnEatgdBEHbyb37I2kd4FRgGzMbAhxb2mdmU4EngAPM\nbKiZzc6UWzaV287MNkzHHS9pGWAPYB0zWx/4Ud42B0GQH9XwVLYBrjGz/wKY2XuSmlNuU2Aw8GA6\nfnHgYeBD4FPgN5JuAm6qVDibT2XVlRbJ6TdB0C4o0t0n4HYz23+hHdLGwLbAXsDRuHAtgJlNAiYB\nDBvSzaprahAEDVGN0Z+7gL1TtwVJfcr2fwwsWaHcI8Dmkgamcj0kDZLUE1jKzG4BjgOGVMHmIAhy\nIndPxcyelXQGcK+kecBTeIC2xOXArzKB2lK5dySNA65KuVTAYywfA3+S1A33Zo7P2+YgCPKjKt0f\nM5sMTG5g37XAtZlNIzP77gKGVyi2cZ72BUFQPWJGbRAEuRKiEgRBrhRp9Cc3Xpq5RJvSF0x7K9Im\nBEFrCU8lCIJcCVEJgiBXQlSCIMiVEJUgCHKlpqIiaVYt6wuCoPaEpxIEQa7URVTknCvpGUlPS9o3\nbe8r6T5J09O+LdP2HSQ9LOmvkq5JzwMFQVBA6uWp7AkMxR8O3A44V1JfYCwwzcxK+6Y3lGelPmYH\nQdAU9Zr8tgVwlZnNA96WdC/+zM/jwGWSugA3mNl0SVtTOc/KAmTzqXRjidp8iiAIFqJQM2rN7D5J\nWwG7AJdL+inwPg3kWSkr+2U+lV7qE/lUgqBO1Kv7cz+wr6TOkpYDtgIek7Qa8LaZXQJcCmxIA3lW\n6mR3EARNUC9P5Xo8l8oMPDn2SWb2b0mHACdKmosnyj64kTwrL9XB7iAImqCmomJmPdN/A05Mf9n9\nFfOwNJJnJQiCghHzVIIgyJUQlSAIcqVQoz9FYafVN2l12WvfuLdNdY9ZedM2lQ+CehOeShAEuRKi\nEgRBroSoBEGQKyEqQRDkSmFERVI3SY9JmiHpWUk/SNsl6QxJL0l6XtKEetsaBEHDFGn0Zw6wjZnN\nSg8UPiDpVmBtYBVgLTP7QtLydbUyCIJGKYyopFm2pcxwXdKfAUcBY83si3Tcf+pjYRAEzaEw3R+A\n9IDhdOA/+JPJjwJfwR8+fELSrZLWaKDs+HTME3OZU0uzgyDIUChRMbN5KUHTysDGktYFugKfmtkw\n4BLgsgbKTjKzYWY2rAtdKx0SBEENKJSolDCzD4C7gZ2AN4Dr0q7rgfXrZVcQBE1TGFGRtJyk3mm5\nO7A98AJwA/DVdNjWRMqDICg0hQnUAn2ByZI642L3RzO7SdIDwBRJx+GB3MPraWQQBI1TGFExs5nA\nBhW2f4CnlwyCoB1QmO5PEASLBoXxVIqEzWn9kHRbUxdMe2t6q8vu2G9om+oOgjwITyUIglwJUQmC\nIFdCVIIgyJUQlSAIcqWqoiJptKST0/L3JU1My5dL2istXyppcDXtCIKgdlR19MfMbgRubOKYmMwW\nBIsQrfZUJPWX9ELyOl6SNEXSdpIelPSypI0ljZN0cRPnuUfSsLS8v6SnJT0j6ZzMMbNSoqYZkh6R\ntEJr7Q6CoLq0tfszEPgJsFb6GwtsAUwEvteSE0nqB5wDbAMMBYZL2j3t7gE8YmZDgPuAI9podxAE\nVaKtovKqmT2dEig9C9yZki09DfRv4bmGA/eY2Ttm9jkwBX9xO8BnwE1p+clK5458KkFQDNoqKtm7\n94vM+hfkG6+Zm8QKYF6lc0c+lSAoBkUaUn4M2FrSsulJ5f2Btr3uLwiCmlOYZ3/M7F9p+PluQMDN\nZvanOpsVBEEL0fxexaJDL/WxTbRtvc1oFfFAYdBeuMOmPpnSvC5Akbo/QRAsAoSoBEGQK4WJqQRO\nW7owbek6tbXuICgRnkoQBLkSohIEQa6EqARBkCshKkEQ5EpVREVSb0nfrMa5gyAoNtXyVHoDC4mK\npBhtCoJFnGqJytnAVyRNl/S4pPsl3Qg8ByDpQEmPpf2/Ts/6IGkHSQ9L+qukayT1TNvPlvScpJmS\nzquSzUEQ5EC1PIeTgXXNbKikkcDNaf1VSWsD+wKbm9lcSb8ADpB0C3AqsJ2ZfSLpO8Dxkn4O7AGs\nZWZWet9yOZLGA+MBurFElT5WEARNUavuyGNm9mpa3hbYCHhcEkB34D/ApsBg4MG0fXHgYeBD4FPg\nN5JuYn5elQUws0nAJPBnf6r2SYIgaJRaiconmWUBk83su9kDJH0NuN3M9i8vLGljXIz2Ao7Gs8MF\nQVBAqhVT+RhYsoF9dwJ7SVoeQFIfSasBjwCbSxqYtveQNCjFVZYys1uA44AhVbI5CIIcqIqnYmbv\npgTYzwCzgbcz+56TdCpwm6ROwFzgW2b2iKRxwFWSSqnbTsUF6k+SuuFezvHVsDkIgnyoWvfHzMY2\nsu9q4OoK2+/Cc9WWs3GOpgVBUEViRm0QBLkSk9EWIWZ98Wmbyqtr6xOG25x4g0HghKcSBEGuhKgE\nQZArISpBEORKiEoQBLlSeFFpzkvegyAoDoUXlSAI2hdVF5VKaQ4k/TK9TP1ZST/IHDtc0kOSZqQy\npan+/ST9RdLLkn5cbZuDIGg9VZ2n0lCaA+AUM3sv5VG5U9L6wAv4LNt9zexxSb3wKf4AQ4EN8BfA\nvyjpIjN7vZq2B0HQOqo9+a2hNAf7pPwniwF98ZQHBvzLzB4HMLOPAFK5O83sw7T+HLAasICoRD6V\nICgG1RaVhdIcSFoduB0YbmbvS7oc6NbEebLTNedRwe7IpxIExaDaMZWF0hwAq+L5VT6UtAKwczr2\nRaCvpOHp2CUjp20QtD+qetM2lOYAeAqPobwOPJiO/UzSvsBFkrrj8ZTtqmlfEAT5U3VPoIE0B480\ncOzjeFrJLJenv9Ixu+ZoXhAEORPzVIIgyJUQlSAIciUCoYsQY1Yu7zm2jGlvPdrqsjv2G9qmuoNF\nh/BUgiDIlRCVIAhyJUQlCIJcCVEJgiBXQlSCIMiVdikq6enmIAgKSC3yqZwu6duZ9TMkHSvpREmP\nS5pZllPlBklPplwr4zPbZ0n6iaQZwGbVtjsIgtZRC0/lMuBggPT8z37Av4E18DcPDgU2krRVOv4w\nM9sIGAZMkLRM2t4DeNTMhpjZA+WVSBqfEj89MZd4B00Q1ItaPPvzmqR3JW0ArIA/TDgc2CEtA/TE\nReY+XEj2SNtXSdvfxVMeXNtIPZH6IAgKQK1m1F4KjANWxD2XbYGzzOzX2YMkjcSfTN7MzP4n6R7m\n51r51Mzm1cjeIAhaSa0CtdcDO+EeyrT0d5ikngCSVko5V5YC3k+CshYLP7EcBEHBqYmnknKl3A18\nkLyN21L+2odTushZwIHAX4AjJT2PJ22qmCIhCILiUhNRSQHaTYG9S9vM7GfAzyocvnOFbZhZz+pY\nFwRBntRiSHkw8Dc8efXL1a4vCIL6UovRn+eAAdWuJwiCYtAuZ9QGQVBcQlSCIMiVEJUgCHIlRCUI\nglwJUQmCIFdCVIIgyJUQlSAIciVEJQiCXFlk3vuTEjqNB+jGEnW2Jgg6LouMp2Jmk8xsmJkN60LX\nepsTBB2WRUZUgiAoBu1OVCTdIqlfve0IgqAy7S6mYmaj6m1DEAQN0+48lSAIik2IShAEuRKiEgRB\nroSoBEGQKyEqQRDkSohKEAS5EqISBEGu1CKbfn9Jz1S7niAIikGhPRVJ7W5yXhB0dGolKp0lXSLp\nWUm3Seou6SuS/iLpSUn3p9ecIulySb+S9CjwY0k9JF0m6TFJT0narUY2B0HQCmrlCawB7G9mR0j6\nIzAGOBQ40sxelrQJ8Atgm3T8ysAIM5sn6UzgLjM7TFJv4DFJd5jZJ9kKIvVBEBSDWonKq2Y2PS0/\nCfQHRgDXpHcpAwvkK7gmvXMZYAdgtKSJab0bsCrwfLYCM5sETALopT6W9wcIgqB51EpU5mSW5wEr\n4C9rH9rA8VkvRMAYM3uxWsYFQZAf9QrUfgS8KmlvADlDGjh2GnCMkksjaYMa2RgEQSuo5+jPAcDX\nJc0AngUaCsD+EOgCzJT0bFoPgqCg1OIF7a8B62bWz8vs3qnC8ePK1mcD36iSeUEQ5Eyh56kEQdD+\nCFEJgiBXYsZq8CU7rb5Jq8te+8a9bap7zMqbtql8UBzCUwmCIFdCVIIgyJUQlSAIcqXQoiLpoXrb\nEARByyi0qJjZiHrbEARByyi0qEialf73lXSfpOmSnpG0Zb1tC4KgMu1lSHksMM3MzpDUGSK3QRAU\nlfYiKo8Dl0nqAtyQSaPwJZFPJQiKQaG7PyXM7D5gK+BN4HJJB1c4ZpKZDTOzYV0WSM0SBEEtaRei\nImk14G0zuwS4FNiwziYFQdAA7aX7MxI4UdJcYBawkKcSBEExKLSomFnP9H8yMLnO5gRB0AzaRfcn\nCIL2Q4hKEAS5UujuT1BbbM6cpg9qgI6cumDaWwvNcGg2O/ZrKPd7+yU8lSAIciVEJQiCXAlRCYIg\nV0JUgiDIlcKISumJ5CAI2jeFEZUgCBYNaiYqkk6UNCEtny/prrS8jaQpafkMSTMkPSJpBUlLSno1\nPZ2MpF7Z9SAIikctPZX7gVJypWFAzyQOWwL3AT2AR8xsSFo/wsw+Bu4Bdknl9gOuM7O55SeXNF7S\nE5KemEvr51sEQdA2aikqTwIbSeoFzAEexsVlS1xwPgNuyhzbPy1fChyalg8Fflvp5JH6IAiKQc1m\n1JrZXEmvAuOAh4CZwFeBgcDzwFwzs3T4vJJtZvagpP6SRgKdzeyZWtkcBEHLqXWg9n5gIt69uR84\nEngqIyYN8TvgShrwUoIgKA71EJW+wMNm9jbwadrWFFOApYGrqmhbEAQ5UNMHCs3sTqBLZn1QZrln\nZnkqMDVTdAtgqpl9UAs7gyBoPYV/SlnSRcDOwKh62xIEQdMUXlTM7Jh62xAEQfMpvKgEQdFpS06U\ntuRiaWvd1SKm6QdBkCshKkEQ5EqIShAEuVJ4UZH0UPrfX9LYetsTBEHjFF5UzGxEWuyPv6g9CIIC\nU3hRySRvOhvYUtJ0ScfV06YgCBqmPQ0pnwxMNLNd621IEAQN055EpVEkjQfGA3RjiTpbEwQdl8J3\nf5pL5FMJgmLQnkTlY2DJehsRBEHjtCdRmQnMSzlsI1AbBAWl8DGVUkqElJd2mzqbEwRBE7QnTyUI\ngnZAiEoQBLlS+O5PECzKtDV1QVtSJ1QrbUJ4KkEQ5EqIShAEuRKiEgRBroSoBEGQK1UVFUlDJY3K\nrI+UNKKxMkEQtG+q7akMZcFXa4wEWiQqkmKEKgjaEU3esJL6A38BHsEF4XH89aM/AJYHDkiH/gzo\nBszGX6T+KnA60F3SFvjbBY/Ep9ofCBwDvAD8Clg1nePb6d3J3we+AgwA/inpR6nOxXEhHGNmL7fh\ncwdBUCWa6wUMBPYGDsNFZSz+1sDRwPeAg4EtzexzSdsBZ5rZGEmnAcPM7GgASd2BWWZ2Xlq/Ejjf\nzB6QtCowDVg71TkY2MLMZqcXiv3MzKZIWhzoXG5gpD4IgmLQXFF51cyeBpD0LHCnmZmkp/E0j0sB\nkyWtARiZV5s2wXbAYEml9V6SSq8/vdHMZqflh4FTJK0MXFfJSzGzScAkP0mfpl74HgRBlWhuTGVO\nZvmLzPoXuDD9ELjbzNYFvoZ3g5pb/6ZmNjT9rWRmpfSRn5QOMrMrca9oNnCLpHiwMAgKSl6B2qWA\nN9PyuMz28hwo5eu34bEVwEeLKp1c0gDg72Z2IfAnYP22mxwEQTXIS1R+DJwl6SkW7FLdjXdvpkva\nF/gzsEda3xKYAAyTNFPSc3ggtxL7AM9Img6sC/wuJ7uDIMgZmS164Yde6mObaNt6mxEEVaeeDxTe\nYVOfNLNh5dtjRm0QBLkSohIEQa4skt0fSe8A/2jkkGWB/7by9G0pW+/yUXfUnWf51cxsuYW2mlmH\n+wOeqEfZepePuqPuWpSP7k8QBLkSohIEQa50VFGZVKey9S4fdUfdVS+/SAZqgyCoHx3VUwmCoEqE\nqARBkCshKu0ESQvlkAmCIhKi0kyUSfpSab3KdW8JHJASVLUb8vqOJK0paek8ztWCOttkez3LZ6+T\neqRjDVFpBpJkKaItaQNJnawFEe42XiCjgCl4aonPW3rOPG5sSaMlbdzScpnvbNk21D0U+APQo7Xn\naEWd2d97mZYKWln5QZJabLuZWUoU/60W1t0LGCOpj6Rd0nKrrgFJfVpTrsOJiqRDJY1rSZnMBXIs\n8H1gpcz5Gv3Byi6wrSQNlDSoGXZK0pLAQcDhZnYn0ENSD0m900XX6O9XVvfBkr4uafum6i47x4nA\nROCj8nM3UmawpM3T8jHAVZJ+K2mVpmwuO8/WwAXA6Wb2Rq26gJnvbCI+rHqzpMMlNZrRMP1m2e/8\neOBioFcrTfkE+Jqkfs05WNJiZvYRnn7kITwlyQ0taQAz5zoBuDCJVIvoUKIi6STgCDzPbnZ7k0ou\naTc8N+8hZva6pJUkdW7q5s5cYN/GM+QdDFwgae2GypTKmdnHwH+AjyWtAtyKJxi/T9I6ZvZFY7Zn\n6t4F+C4wBNgv3SxNImkDYLSZbQW8ImkTSXtkz12hTHdgF2B8uql2xRNxLQ6cCqzXAmF5G9gc2DHV\nOa9W3U5JewPbm9mYZMf2Zja3iWKdM9/5AXhe573N7F+SVpS0YgvN+Bvuoa6cztng9yZpOeDatPov\nYGngjfS/0bIVznUksCdwopl9lBqy5qaI7TiiIk+svRF+kb6dXPqzoPINUrp4Mz/GasBfgTUk/QC4\nBpiRWocvmqh7CDDKzLYG+uAt0ItqIEYiaR1J26QW6hM8l++ewJVmdjj+ZoEbJS3bVCsk6Qhgf2Bb\nM5uQ7F4ztURNMRtYUtKZwPm4x/JbSYc0UJfM8wrfiH9XWwP3m9kLuJh+CnwTGNqEp9NH0jKp3BBg\nbBKoUrcg9+s2a09a/gj4uaSTge7AgWnfag2UXw64LnOezsBNwChJpwI3AGdIWqsJO4ZLul7SOsAs\n4C7gPEndG7vOzOwdYH9JI3EvZVCq/9eS1k0N0NqSujZR/1J4junzgQHJO78JOFpS7+aIeocQFfkL\nzI4AhgOXAxfh7yDaVdI5FY5X5mZdPv3/Pf7akNOB5/EbfQYVUltW+OLnAc8nD2EAcFC6QLaR1Lus\n7M7460yOBc4DbsZfg3IonkkPMzsfuIcKicsr1N0Z97A2S+sP4C3aMEkTysunc2wtT2L+AXA83t2b\nbGZ7AydSIQdx2Xf2NnA1LizbS9razOaZ2bHJ5kNwz6VS3bvjMaQpksaa2XPAxsAJkr6XPn+jIt4U\n5WJe1mU5Evg/oC9wCn7N7GJmcyUdh9/gXcu/53RT75c+bx/gMaAf/oaHGcAJeDrVhQL+ZSL5BO6h\nHIoL0du4tzIoHd+YV/w/3DN5IW36BfAocE5qCE+nkdiUpKPwa25xYF/gTOBD/LdcE5jXrK5UW55i\nLPof0Cn9H4e73j/BuyB90/bd8D77Yg2UPwq/qc8EvlV2zt2A50rnypRRZnl73MNZHG9xXszsOwK4\nA+id2TYSeAnYOK3/GU+fuRrwT7wL0w9vNWcCyzdS9+rAEmn5APyC3iStL4mLYt8Kn/nbwL3pe/oL\nsGJm39eBZ4G1GvnOj8VzD3fF37RwAv5up60yx6zQQNnt8JtxOfyGeCPzva8DvJ8+V6c2XBPdgCuB\nrhX2jUifebG0PjXZvj3uYc0E1mni/LsBrwFLpvXSbzAaeApPF1A6tmfZZ58AbJ7ZNhb3LN8Bft+C\nz7gT8DKwVFofn67jBm0HtkzXY5e0PqBkH96dfbT8emvwXHnexEX7A9ZI/zunG+t04Gi8tTgceAZY\nt4Gy+6Sba5V0c1+atnfB+8ovNlQ2HXdMuggHpPVReNBvSrrRniovj7/z6KtpeUW8b3wzHhw+HbgC\nOBu4v4kL5NvpArkNb3V7463oO8CIdIwqlBsC3JaWL043VSdchNYCHmziM4/FRWFgZtvAZM+U7A1T\nVq70uMgB+I09OtV1EPAK8L20v0dO18VC5wHWwAXlHmDltK07cA7eGE0GBjfz/Dsnu5dO6/vj3sq6\nWRtw8dkd9wRfAC4FfoO/qK9bOq53+g7vADZqwWcchXvUfdL6QiKaObYfcEk6fqPM9u54g9zgfVLx\nfHnexEX6w996+Bre1QB3uw8Cbsdb4ZuzFwnu5g7NrB+IxwQOTTdnScEHAysAqzZS99a4aJRaiiF4\nSzQA9zaOAtZuwv5TgFPT8uHAL4H+aX3psmMXyyyPSBdwbzzAeQL+IjZwL+JVvLXOejWlm3oocCEe\nO7klc2HvhAtLzyZsPgrYr3TTZLYPwFvhhTyjtL9XZrkL3joPT+u/xW/0lXK+PiqJ6lb42xoOIHlT\nme9m8Raef2dcKHrjDcTKFY7ZBng3fcbN07aR6Tf4IRkhwLvtO7TQht2A6XjDUNG7Y75XtirulZ0C\nDCp9dlz0BrSo3jx/qKL94e8g+iuwf2bbNOA0ylxwXET6Asuk9V3xrFf3ZY45CjiDJDANXaB47OU8\nPHZzLt56X4n3zVv7WW7F3/a4QH24GH6D+d2ynYBrM/uHphtlg7S+dIVzL5/+d8WDci9l9o2nrJtW\n6TOnbd/BX/zWJbPtINzLaeii3gX3wM7CW9fO+AjXxXi34xb83VDVukaOxoOSl6TffxT+tob9ybj7\nlT5vM869Oz7SWLEsLtSPAHPxkZbSb7BVsuesJAir417zmq2wocGGAI+XXYY3mkOADdN1+12a6ZVV\nPG+1fqyi/KWLZCb+ytY9cU+lX2Z/tpVfGe87jsTfZXQW7vaug3sslbos2Rt8NdxV7ZWO/3m66Xvg\nLc/4ZtpcLlJjgCfJxDfS9k7AMrjnNDjVu0z6jAdnjpsMjC2VKTvHN4HrgR/hMZPReMv5O+Z309Zr\n5DPvCOyFj2p1TRflH9N3cQgedxrUwOccnvavkW6aK3CXe0s81jUD+FoVr42jcMEcgLfoF6Tte+NC\nvHf599WKOnqWrWe/u6HAybhn+D9gr7R98XQNZj3phRqDNtpV8sC7pWv+8rR9BN5NP4EWemdfnrta\nP1iR/vDuyD14qzcks31Z5gcvD8Yj5xPxGMrm6UY9AfcSrmbBPnH5jX8iHut4LgnIxpl9++AtVoMB\nzgbs7sr84Gi5mK2Hewar4V27+3ARXC7Vdxne2pdu7NUqnH/fVK4vPgx5NrAEHkf6P5ropuGxkgdw\nt/k23DNcPQnCn3CvsGLsB3e3J+Cxns1wb65/2tcv/V+gC5LDddApez7g/6Vr4AS8O9yV+d2Bnck0\nPjlfj5sA30jL5+PiNRzvCo0ts7FzTnWWX6/fTNf3xHR9L443UovjjWjFYHqz6qrGl1bEv3SzdC9b\n74N3S67DA6/LpX0n4N2AUj+3M2UjRJmLrxM+5PlgurlXwIN7Z+Lu7UhczNZrhc1dcE9rzbLtnfFR\noanAcemzrJRu5JPTjb0OPoJyVkN1412b7XFBvZ3UMjV0M7FgKzsSuDktn5i+v98AO2WO6dbAeVbA\nu4Vj8VGxp0kxE9zrOTd9l7mISYX6B6XzT8Ybmz9kfs9jgCOqeB12Sb/LR3gXa098lGvD9J3OxmMw\nbfKQyq9V5jeeO6Zr44f48PUVJOFK19KP2vq91+ymLtIf3scvtRQHAe8Bp5UdczzuFm5WofyyeBC4\nFFnfBLiT+YHZFXB3egzuzufmuibbT8G9qsH4/JmTgJ54FP9m3MsoDWVWin2cBOyBt8avAfdm9k3A\nW/ByEc0KynJ493AgHnu6Exe6y9Ln/lpjF2Y69gZcFH+JC+HaeGs9A9g15997BPMDyMfgw60/wUfV\n3gWOTPvG0Uh3LQc7Vknf3QDcc52CdzcfS99hp9I1lXO9g3Cv9gbghbRtmfRZz0jLh+GjPC3ypiv9\ndYjJbxXojM9+XANvMXYCRkg6tjQZzcx+ikfh3ywvbGb/xS/Oh9NEpyfxUZWt00zQt3Hvp7uZzTaz\n9/MwOk18WhK/OMcDb+FdlvVwd/ZDfKRoB+DYsglppXPsjLeIT+It1S3AQ5LWk3QgfmNNNbPPs+VK\n55F0NN7Cn4SL0ibAXWY2Dw86Pg88Ul5vKruSpDXTsROS/c/i3Y5f4mJ2mpndlPN0/KXx1/J+Hw9I\n7oRPMCvN/v2OpIvxuUN7mdlLOdYNfPn4wjfwmFM/vFv7Fu7hXYeL6lfM7L10fB4PgnYGSJ9nKTxW\n9UdJ3czsXfx72BD3ZvcC9jGfxdy2eiv89oss8qeLv0jLW+Ct7Od4d2U1XET+gLufmwC7ld9cZefb\nOZUZig8PjsZbm1fwm3tHM8pgb7oAAAWASURBVHs5J9vXwqfa/1z+sN6+wL9xV3olvLWfgcc3euIj\nMK+n2aMDzew5+YOUJwN/M7Nd03m3BDbFhegD/OG9pxuwYXfcgxuNt3qP46J0CT7bd1Ngz0qfWf6k\n7o/wm/oP+CjRN/FJXQ/JH57sYmbvVRLDtiJ/kPKnuOAdkaarj8EFuhc+4jTHzD7Ms94yG5bCvbFf\n4MHxpYHzzex5Saua2T9zrKszLvrv4QMQK+GjmWvhkwivMbN/yJ8pexP3bGflUndHEpUSaTryVvgo\nxWZ4i3UR3m05Ev8BfmhmTzXjXKPw1mcY3gJtjwc+r8izxZOnAHgTd1XfwoetD2W+sPTD4zh3ARdl\nPIuBaf+/8ODoZXiA9fdmdkHm/N1wh2ROIzYcgs/M7YXHQ3Y1s8/kD98NBq5q7DOnOgbjrvjMZMdr\nuBC93sKvpMXIHwq9BJhgZn9Int84fPTpHDP7oNo2JDsG4UHwscA/zGxj+cOp83KsowfwVTx20gsf\noJiVGoaRwN9x72UF4ITGfvcWU42+Y5H/8Fb2adLkNdwjORefu9I/basYYGzknLvg/dHSDMo8g2yd\nMsu9cc/oPNwb2TCtfwdv9daiQpA1Hf8RcFRa3xn3NCa00JatcS/s/sy2Y/FHIJo9/Jgu5uXx2M+1\npLgVVQrMVvitZjI/xtKJNKW+xtfhErjXslUV6xiOP95xNe7llraPAr6Hd1fXz7veDueppAfG+pjZ\nmfInjD+XNByfRfk63sp/1orz7o4H/jaEtj/0VuH8o/GnfNfEY0Ir4kG2gXh85xncla4UyxiIe2TH\nA2eb2dWSNsI9mAvM7Kpm2tATf1zgC3x0bFVcVA4xs2da+blOwYe7x7emfCvr3Bmfi3GcmU2tVb2N\n2FON7l53M5stz4cyCu/e3mJmUzOxxI/NH0LMlZqnmisA/wB2l3Stmb2YtvXDHzOf3BpBATCzGyTd\nkZeYlD05ux/e578EvzhuxZ9ePRmPB10AvN3QhWlmfwP+JukD/PH7D/BJT5/hrVWzMHefz8W9vRPx\nkZNxrRGUzOd7BdiqdBO09DytwcxulXRYqrvuVEFQJuADD5/gkxivxr3DXVIXcEV8lnnuggIdMKaS\nlPtEXFAfxL/sY3F3+O/1tK1EmaCsinsZT5jZK5L2wbsbU/Egcxc8uNqsH1LSTnh37xPg62b2bCtt\n7AJgTScuauwcwoPlr7bW0wkWRJ5+cm883nY+PohwjJn9WdJ2+MTIC6v5fXc4UQGQ1Bd/2Go0Pgx7\nlpnNrK9VTpmgTMC7ZUviIxdXmNmnSVh+gndffm1pGLIFdSyPN5Dv5Gt9UGvKrpeuuJhcgwegN8e9\nlLPwYOz1NbGpI4pKiVKyntZ2eapJitHsiQ/DHo7P5bgeeCDFgfYEphfFuwrqS+rWzE5/j+GPSOxt\nZu9IugMf5RkBzMq7u1VOR538BriYFFRQVsJHdeaaD9GehgfWxgBfTQHm60JQAvgy5vZrfK7UT5mf\n16avpMPx0a7tzOzjagsKdHBPpcgkT+Ri3G29Sv7+lh/jIy+nVSvIFrQvKsTcxuIB/FJemgOAPWrZ\nve+Ioz/tAjO7TtIcfHo5SVhOwufChKAEC8XcJL1pZldKmoWn3XgKONf8rQw1I0SlwJjZzZK+ACZJ\n+tzMSvlKgw5OirkNI70XCn/+a1NJD5jZjWn28oxaCwpE96ddkJ5beSViKAF8GXN7GLjdzL6eBOQU\nfMb1jcDd1sgza9WmQwdq2wtmdnsISlDCzN7En5vaWdL+ZvYpnix7Lp4vpa7v3I7uTxC0Q4occwtR\nCYJ2SlFjbhFTCYJ2TtFibiEqQRDkSgRqgyDIlRCVIAhyJUQlCIJcCVEJgiBXQlSCIMiVEJUgCHIl\nRCUIglz5/+X/D8Ex0TOeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = \"us cities lose 36 million trees each year . here is why it matters \"\n",
    "plot_attn_heatmap(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGh2jq9tG50n"
   },
   "source": [
    "Each row of these plots represents the attention weights on the history tokens when the model is trying to predict the next word. For example, the third row of the first plot can be interpreted as the attention weights over \"top\" and \"warning\" when predicting \"signs\"; you'll note that the rest of the row is black (i.e., zero attention on future words). Are these attention maps interpretable? If you (as a human) were solving the same word prediction problem, would you focus on the same words as the ATTNLM does?\n",
    "\n",
    "  * Yes, the attention maps are interpretable and provide insights on the next word that can appear, given\n",
    "a history word token. As a human, if I were solving the word prediction problem, I might not always predict the\n",
    "words as those focused by the ATTNLM. This is because, my training dataset would be larger as compared to\n",
    "the one used, and the next word attention weights might be different. For example, given the history word token\n",
    "million, looking at the heat map, I would predict the next word trees with a greater probability, since the attention\n",
    "weight for the combination million trees is higher as compared to the other alternatives. But, as a human I can\n",
    "also predict million cities, million year, which according to the heatmap has zero attention weights. Also, I would\n",
    "use the attention weights calculated for some of the words in the heatmap. For example, given the history word\n",
    "million, I would not predict the next word token to be either lose, here, since the attention weights for the\n",
    "combination is zero, which is also intuitive from english grammar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EhxYhx2HyPY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RX8B91XHDsJd"
   },
   "source": [
    "## Q3. AllenNLP and Probe Tasks (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojgaFW4gGb-6"
   },
   "source": [
    "### Overview\n",
    "In this homework, we will be studying probe tasks. Probe tasks are special tasks designed to interpret neural networks (especially, deep networks or word embeddings in NLP). Probe tasks generally use simple classifiers and special datasets to analyze the linguistic content stored in dense representations.\n",
    "\n",
    "We will also use this assignment to learn AllenNLP (https://github.com/allenai/allennlp), an excellent tool to build deep models for NLP and seamlessly integrate pretrained word embeddings.\n",
    "(NOTE - This assignment is written using `allennlp` as a Python library. A faster way of using `allennlp` is via JSONNET configuration files, as described [here](https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/walk_through_allennlp/configuration.md). However, learning to use AllenNLP as a library is essential when you want to write custom AllenNLP modules. [This](https://allennlp.org/tutorials) is a good tutorial on using AllenNLP as a library.)\n",
    "\n",
    "Let's start by setting up Google Drive to download data.\n",
    "\n",
    "(*Run the cell below. No need to edit any code here.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "The2yi02jscj",
    "outputId": "c12c5691-edab-48df-c857-f012808a3e4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6BhHo6LZPrqN"
   },
   "source": [
    "### Install AllenNLP\n",
    "\n",
    "Run the below cell once per session to install `allennlp` locally. You might need to restart the runtime after doing this. No need to re-run it for a different runtime in the same session.\n",
    "\n",
    "(*Run the cell below. No need to edit any code here.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cjRB5xxFOoZh"
   },
   "outputs": [],
   "source": [
    "# can take about a minute\n",
    "!pip install allennlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93Hzae6bP3w1"
   },
   "source": [
    "### Subject-Verb Agreement\n",
    "\n",
    "In this assignment, we will design a probe task to test whether word embeddings capture subject-verb agreement. For instance, in English we use the singular verb in this sentence,\n",
    "\n",
    "\n",
    "*   CORRECT - This assignment **is** very easy.\n",
    "*   WRONG - This assignment **are** very easy.\n",
    "\n",
    "Subject-Verb agreement is an important probe task in NLP literature (see [Linzen et al. 2016](https://arxiv.org/abs/1611.01368) and [Gulordova et al. 2018](https://arxiv.org/abs/1803.11138)). We will be using the evaluation set described in  [Linzen et al. 2016](https://arxiv.org/abs/1611.01368), but formulating the problem in a different way.\n",
    "\n",
    "We start by downloading our dataset. Each `inp` variable represents an English sentence. Each `out` variable has four parts, a) index of verb b) correct verb form c) wrong verb form d) number of agreement attractors (we will not need this data for our experiments, but you should read about them in [Linzen et al. 2016](https://arxiv.org/abs/1611.01368) to help you explain some of the results we obtain).\n",
    "\n",
    "(*Run the cell below. No need to edit any code here.*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "9P4IoAcQzjn1",
    "outputId": "dc9bfeb9-d6e6-4d16-d199-f47a8178c42b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of all data = 7489\n",
      "train, valid data lengths = 748, 1124\n",
      "('the two versions of the game have identical core gameplay elements , though the extreme version is tweaked to feel more like the arcade mode in the original off-world interceptor . <eos>', '6\\thave\\thas\\t1')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "f_agreement = drive.CreateFile({'id': '1S1_RQQHTwwf0F6IM2MmbIr4inINyNbsQ'})\n",
    "f_agreement.GetContentFile('./agreement.pkl') \n",
    "\n",
    "\n",
    "with open('./agreement.pkl', 'rb') as f_in:\n",
    "    agreement_inp, agreement_output = pickle.load(f_in)\n",
    "\n",
    "\n",
    "agreement_inp = agreement_inp.strip().split('\\n')\n",
    "agreement_output = agreement_output.strip().split('\\n')\n",
    "\n",
    "data_points = [(inp, out) for inp, out in zip(agreement_inp, agreement_output)]\n",
    "print(\"Length of all data = %d\" % len(data_points))\n",
    "\n",
    "# Splitting into 10%-15% train-valid splits. We are not using the full dataset for computational reasons.\n",
    "# We take a small training dataset since we want to assess the linguistic knowledge stored in pretrained embeddings.\n",
    "len_data = len(data_points)\n",
    "train_data = data_points[0 : int(0.1 * len_data)]\n",
    "valid_data = data_points[int(0.1 * len_data) : int(0.25 * len_data)]\n",
    "# Actual dataset sizes will be two times this, both the positive and negative example of these sentences\n",
    "print(\"train, valid data lengths = %d, %d\" % (len(train_data), len(valid_data)))\n",
    "# print(train_data)\n",
    "print(train_data[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMu4St9OUVkv"
   },
   "source": [
    "### AllenNLP DatasetReader\n",
    "We next write a `DatasetReader` in `allennlp`, which is essentially a PyTorch `DatasetReader` with some syntactic sugar. The use of `Field` objects is essential to allow seamless padding and integration with rest of the `allennlp` pipeline.\n",
    "\n",
    "We will model our probe task as a binary classification task, where given an input sentence, a network has to determine whether the subject-verb agreement is correct or wrong. Notice how we generate two data points per sentence  in the `_read()` function, one with the correct verb (labelled `\"correct\"`) and the other with the wrong verb (labelled `\"wrong\"`).\n",
    "\n",
    "(*Run the cell below. No need to edit any code here.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h96J8X8Fh4qh"
   },
   "outputs": [],
   "source": [
    "import allennlp\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, LabelField\n",
    "\n",
    "from allennlp.data.token_indexers import TokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from typing import Iterator, List, Dict\n",
    "\n",
    "class AgreementDatasetReader(DatasetReader):\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    def text_to_instance(self, tokens: List[str], label: str) -> Instance:\n",
    "        tokens = [Token(x) for x in tokens]\n",
    "        fields = {\n",
    "            \"sentence\": TextField(tokens, self.token_indexers),\n",
    "            \"labels\": LabelField(label)\n",
    "        }\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, dataset) -> Iterator[Instance]:\n",
    "        for inp, out in dataset:\n",
    "            correct_input = [x for x in inp.split()[:-1]]\n",
    "            position, correct, wrong, _ = out.split('\\t')\n",
    "            position = int(position)\n",
    "            # verifying whether input is in correct form\n",
    "            assert correct_input[position] == correct.lower()\n",
    "            # yield both the correct and wrong forms of the agreement\n",
    "            wrong_input = [x for x in correct_input]\n",
    "            wrong_input[position] = wrong.lower()\n",
    "            \n",
    "            yield self.text_to_instance(correct_input, \"correct\")\n",
    "            yield self.text_to_instance(wrong_input, \"wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBWTOqrnVgRC"
   },
   "source": [
    "### Q3.1 - AllenNLP Model (10 points)\n",
    "\n",
    "We will now build a simple one-layer classifer on top of average-pooled embeddings.  Notice how the `word_embeddings` are passed as a parameter to the model. This helps abstract the word embeddings outside the model, so it is very simple to swap random vectors with word2vec, GloVE, ELMo or BERT (as we will do in this exercise).\n",
    "\n",
    "Implement the `forward()` function for this model. Make sure you implement masking correctly, to avoid including masked vectors in the average pooling.\n",
    "\n",
    "(*Implement the missing sections in the code below. This is the only code you need to implement, so be careful!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AC9yxSuVi9Nd"
   },
   "outputs": [],
   "source": [
    "# requires code\n",
    "import torch\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "\n",
    "class AgreementProbeTask(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        self.final_linear = torch.nn.Linear(\n",
    "            in_features=word_embeddings.get_output_dim(),\n",
    "            out_features=vocab.get_vocab_size('labels')\n",
    "        )\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "    def forward(self,\n",
    "                sentence: Dict[str, torch.Tensor],\n",
    "                labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        mask = get_text_field_mask(sentence)\n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "\n",
    "        # obtain a single vector for each element of the minibatch from the embeddings matrix\n",
    "        if 'bert' in self.word_embeddings._token_embedders:\n",
    "            # For BERT, we use the first token [CLS] for classification\n",
    "            # construct logits using the first embedding vector of the sequence only\n",
    "            #\n",
    "            # ENTER CODE HERE 1\n",
    "            encoder_out = embeddings[:,0,:]\n",
    "        else:\n",
    "            # For other models, we will average the embeddings across the sequence dimension\n",
    "            # Use the `mask` variable to correctly normalize the sum of embeddings by the sequence lengths\n",
    "            #\n",
    "            # ENTER CODE HERE 2\n",
    "            encoder_out = torch.div(torch.sum(embeddings, dim=1), torch.sum(mask, dim=1).float().view(-1,1))\n",
    "        # Use the linear layer declared in __init__ to construct the logits\n",
    "        # \n",
    "        # ENTER CODE HERE 3\n",
    "        logits = self.final_linear(encoder_out)\n",
    "\n",
    "        output = {\"logits\": logits}\n",
    "\n",
    "        if labels is not None:\n",
    "            self.accuracy(logits, labels)\n",
    "            output[\"loss\"] = self.criterion(logits, labels)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6iqZKhxBT99z"
   },
   "source": [
    "### A Primer on Fine-Tuning\n",
    "\n",
    "Before we analyze some word embeddings, we should understand some terminology (used in rest of the assignment). While using pre-trained weights for a downstream task, typically two approaches are used - 1) they are kept constant during the training 2) they optimized jointly with the rest of the network on the downstream task objective. The first approach (we will call this **frozen embeddings**) saves training cost and is arguably a better indicator of the native linguistic knowledge stored in the original embeddings. The second approach (we will call this **tunable embeddings**) generally leads to the best performance on the downstream task, but often suffers from catastrophic forgetting.\n",
    "\n",
    "It is an open research problem to fully understand the cases where fine-tuning word embeddings is useful.  [Peters et al. 2019](https://arxiv.org/abs/1903.05987) is a recent research paper on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVbmONVWS6-Q"
   },
   "source": [
    "### Putting the Pieces Together ...\n",
    "\n",
    "The code below is an implementation of the training infrastructure in AllenNLP, specific to our application. For the rest of the assignment, you only need to worry about modifying `config` and explaining the results you get. Run the cell below once to register the functions.\n",
    "\n",
    "*(Run the cell below. No need to edit any code here.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pa97L8s7Tcf"
   },
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import (\n",
    "    Embedding,\n",
    "    ElmoTokenEmbedder,\n",
    "    PretrainedBertEmbedder\n",
    ")\n",
    "from allennlp.modules.token_embedders.embedding import _read_pretrained_embeddings_file\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "from allennlp.data.token_indexers import (\n",
    "    SingleIdTokenIndexer,\n",
    "    ELMoTokenCharactersIndexer,\n",
    "    PretrainedBertIndexer\n",
    ")\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def reset_weights_fn(m):\n",
    "    if hasattr(m, \"reset_parameters\"):\n",
    "        m.reset_parameters()\n",
    "\n",
    "def run_experiment(config):\n",
    "    embedding_type = config['embedding_type']\n",
    "    embedding_size = config['embedding_size']\n",
    "    urls = config['urls']\n",
    "    tunable = config['tunable']\n",
    "    batch_size = config['batch_size']\n",
    "    num_epochs = config['num_epochs']\n",
    "    reset_weights = config['reset_weights']\n",
    "    \n",
    "    # A token indexer is necessary to inform the DatasetReader the indexing process\n",
    "    if embedding_type in ['random', 'glove']:\n",
    "        logger.info(\"Using a single ID token indexer...\")\n",
    "        token_indexers = {\n",
    "            'tokens': SingleIdTokenIndexer()\n",
    "        }\n",
    "\n",
    "    elif embedding_type == 'elmo':\n",
    "        logger.info(\"Using elmo character token indexer...\")\n",
    "        token_indexers = {\n",
    "            'elmo': ELMoTokenCharactersIndexer()\n",
    "        }\n",
    "\n",
    "    elif embedding_type == 'bert':\n",
    "        logger.info(\"Using bert token indexer...\")\n",
    "        token_indexers = {\n",
    "            'bert': PretrainedBertIndexer('bert-base-uncased')\n",
    "        } \n",
    "\n",
    "    else:\n",
    "        logger.info(\"Invalid embeddings type, quitting...\")\n",
    "        return\n",
    "\n",
    "    # Loading training and validation datasets via our reader\n",
    "    reader = AgreementDatasetReader(token_indexers)\n",
    "    train_dataset = reader.read(train_data)\n",
    "    valid_dataset = reader.read(valid_data)\n",
    "    # `vocab` contains both the input vocab and output label space\n",
    "    vocab = Vocabulary.from_instances(train_dataset + valid_dataset)\n",
    "\n",
    "    if embedding_type == 'random':\n",
    "        logger.info(\"Using random embeddings...\")\n",
    "        token_embedding = Embedding(\n",
    "            num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "            embedding_dim=embedding_size,\n",
    "            trainable=tunable\n",
    "        )\n",
    "        word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "    elif embedding_type == 'glove':\n",
    "        logger.info(\"Using glove embeddings...\")\n",
    "        weight = _read_pretrained_embeddings_file(\n",
    "            urls[0],\n",
    "            embedding_size,\n",
    "            vocab\n",
    "        )\n",
    "        token_embedding = Embedding(\n",
    "            num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "            weight=weight,\n",
    "            embedding_dim=embedding_size,\n",
    "            trainable=tunable\n",
    "        )\n",
    "        word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})   \n",
    "\n",
    "    elif embedding_type == 'elmo':\n",
    "        logger.info(\"Using elmo embeddings...\")\n",
    "        elmo_token_embedding = ElmoTokenEmbedder(\n",
    "            urls[0], urls[1], dropout=0, requires_grad=tunable\n",
    "        )\n",
    "        if reset_weights is True:\n",
    "            logger.info(\"Resetting elmo weights...\")\n",
    "            \n",
    "            elmo_token_embedding.apply(reset_weights_fn)\n",
    "        word_embeddings = BasicTextFieldEmbedder({\"elmo\": elmo_token_embedding})\n",
    "    \n",
    "    elif embedding_type == 'bert':\n",
    "        logger.info(\"Using bert embeddings...\")\n",
    "        bert_token_embedding = PretrainedBertEmbedder(\n",
    "            'bert-base-uncased', requires_grad=tunable\n",
    "        )\n",
    "        if reset_weights is True:\n",
    "            logger.info(\"Resetting bert weights...\")\n",
    "            bert_token_embedding.apply(reset_weights_fn)\n",
    "        word_embeddings = BasicTextFieldEmbedder(\n",
    "            {\"bert\": bert_token_embedding},\n",
    "            {\"bert\": ['bert']},\n",
    "            allow_unmatched_keys=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        logger.info(\"Invalid embeddings type, quitting...\")\n",
    "        return\n",
    "\n",
    "\n",
    "    model = AgreementProbeTask(word_embeddings, vocab)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        cuda_device = 0\n",
    "        model = model.cuda(cuda_device)\n",
    "    else:\n",
    "        cuda_device = -1\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # A bucket iterator is needed to sort the data by sequence length and reduce padding overhead\n",
    "    iterator = BucketIterator(batch_size=batch_size,\n",
    "                              sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
    "    iterator.index_with(vocab)\n",
    "\n",
    "    # This function wraps all the whole training loop into a single object\n",
    "    trainer = Trainer(model=model,\n",
    "                      optimizer=optimizer,\n",
    "                      iterator=iterator,\n",
    "                      train_dataset=train_dataset,\n",
    "                      validation_dataset=valid_dataset,\n",
    "                      patience=20,\n",
    "                      validation_metric='+accuracy',\n",
    "                      num_epochs=num_epochs,\n",
    "                      cuda_device=cuda_device)\n",
    "    return trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ooop9x9DJE3"
   },
   "source": [
    "###  Q3.2 - GloVE Embeddings (5 points)\n",
    "\n",
    "We are ready to train our model! We will start by training a baseline classification model which will use GLoVE embeddings. We will test both cases where the GLoVE embeddings are trainable and fixed. \n",
    "\n",
    "1. Try varying the embedding size (50, 100, 300) and report **early stopping** validation accuracy in the text box below. AllenNLP will provide this information to you at the end of training. Keep the embeddings frozen.\n",
    "2. Next, report the performance for tunable embeddings.\n",
    "3. Explain your observations / trends. What do you notice about training time / number of epochs across different runs? You are encouraged to think critically about the trends you notice in this question and the following questions. Most of the points awarded will depend on your explanations.\n",
    "\n",
    "(NOTE - The loss and accuracy values are in a similar range in some parts of this assignment, read the logs carefully!)\n",
    "\n",
    "*Hint - As a verification of your implementation, your 300 dimensional tunable accuracy should about 61%. This could vary slightly due to stochasticity.*\n",
    "\n",
    "\n",
    "The three sizes of GloVE embeddings that we are going to use can be found here -\n",
    "\n",
    "\n",
    "*  50 dimensional = https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.50d.txt.gz\n",
    "*  100 dimensional = https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz\n",
    "*  300 dimensional = https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz\n",
    "\n",
    "**For your reference, the largest GloVe setting (300, tunable) takes about 3 minutes 30 seconds to finish finetuning**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KON2Vth8bWlZ"
   },
   "source": [
    "### Enter your answers here\n",
    "\n",
    "\n",
    "**Frozen Embeddings**  \n",
    "50 dimensional = 0.5533807829181495\n",
    "\n",
    "100 dimensional =  0.568950177935943\n",
    "\n",
    "300 dimensional =  0.5720640569395018\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Tunable Embeddings**  \n",
    "50 dimensional =  0.5769572953736655\n",
    "\n",
    "100 dimensional =  0.5945302431001103\n",
    "\n",
    "300 dimensional =  0.604982206405694\n",
    "\n",
    "**Explain your results here**\n",
    "We can see that the early stopping validation accuracies increases with the increase in word embedding size for both frozen as well as tunable embeddings. Its also observed that the tunable embeddings with 300 dimensions takes lower epoch to converge to optimal solution as compared to frozen embeddings. We can safely say here that the model performs better when the embeddings are tuned to the training data as compared to the pretrained embeddings tuned on a generic data. The increase in the validation accuracy post tuning the embeddings can be observed by comparing the individual accuracies for each dimensions of the word embeddings between frozen and tunable. We can also obsserve that the number of epochs and the training time decrease with the increase in dimensions of the tunable embedding model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MvQuWDP6Q89-"
   },
   "outputs": [],
   "source": [
    " config = {\n",
    "    'embedding_type': 'glove',\n",
    "    'embedding_size': 300,\n",
    "    'urls': [\n",
    "        # 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.50d.txt.gz'\n",
    "        # 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz'\n",
    "        'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz'\n",
    "    ],\n",
    "    'tunable': True,\n",
    "    'batch_size': 100,\n",
    "    'num_epochs': 100,\n",
    "    'reset_weights': True\n",
    "}\n",
    "run_experiment(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0oVb5-e9LKck"
   },
   "source": [
    "###  Q3.3 - Contextualized Word Embeddings - ELMo (5 points)\n",
    "\n",
    "Having studied standard word embeddings, we are now going to explore modern sentence embedding techniques such as ELMo.\n",
    "\n",
    "1. Run the model with frozen and tunable ELMo embeddings and report the **early stopping** validation accuracy results as in Question 1.2.\n",
    "2. Run a baseline ELMo model with randomly initialized weights (Use the `reset_weights` configuration) and report the results. Consider both settings - frozen embeddings and tunable embeddings.\n",
    "3. Explain your results, compare it with GloVE embeddings. Why was this model so slow?\n",
    "\n",
    "**For your reference, the worst case of ELMo finetuning (random weights, tunable) takes about 1 hour and 20 minutes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYGmO-lskZl1"
   },
   "source": [
    "### Enter your answers here\n",
    "\n",
    "\n",
    "\n",
    "**Frozen Embeddings**  \n",
    "random weights =  0.5440391459074733\n",
    "\n",
    "pretrained weights = 0.6619217081850534\n",
    "\n",
    "\n",
    "\n",
    "**Tunable Embeddings**  \n",
    "random weights =  0.5057829181494662\n",
    "\n",
    "pretrained weights = 0.8830071174377224\n",
    "\n",
    "\n",
    "**Explain your results here**\n",
    "\n",
    " * In ELMo models, representations are character based while GloVE embeddings are word-based, which implies higher number of trainable parameters in ELMo models as to GloVE embedding models. Hence ELMo models are slower to train than GloVE embedding models.Thus the number of epochs and training time are higher for ELMo models.\n",
    " * It's observed that the validation accuracy is higher for ELMo as compared to GloVE embedding models in both frozen embeddings and tunable embeddings. This is perhaps because the word embeddings in ELMo account for the distribution of the context in the sentence, which is not accounted for in GloVE embedding models.\n",
    " * We can also observe that the validation accuracy is higher for pretrained weights in both cases when the embeddings are frozen and tunable. We can say that the ELMo was better pretrained as compared to our training dataset.\n",
    " * It can be seen that pretrained weights with tunable embeddings have a higher validation accuracy than the pretrained weights with fixed embeddings. Its expected since the model is tuned to a particular task of Subject-Verb agreement which would have a better accuracy than the model trained in a generic manner.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TaQkdCdkzIe2"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'embedding_type': 'elmo',\n",
    "    'embedding_size': 1024,\n",
    "    'urls': [\n",
    "        'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json',\n",
    "        'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n",
    "    ],\n",
    "    'tunable': True,\n",
    "    'batch_size': 100,\n",
    "    'num_epochs': 100,\n",
    "    'reset_weights': False\n",
    "}\n",
    "\n",
    "run_experiment(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CetK-QJtOGZZ"
   },
   "source": [
    "###  Q3.4 - Contextualized Word Embeddings - BERT (5 points)\n",
    "\n",
    "As our final step, we will analyze representations learnt from BERT.\n",
    "\n",
    "1. Run the model with frozen BERT embeddings and report the **early stopping** validation accuracy as earlier.\n",
    "2. Run a baseline BERT model with randomly initialized weights (Use the `reset_weights` configuration). Consider only the frozen weight setting.\n",
    "3. Explain your results, compare it with GloVE and ELMo embeddings. Why was this faster (or slower) than ELMo?\n",
    "\n",
    "\n",
    "\n",
    "(Note - Fine-tuning BERT was crashing colab, probably due to a memory error. Feel free to check if this is the case and find any workarounds for this. Maybe, vary the batch_size)\n",
    "\n",
    "**For your references, finetuning BERT with randomly initialized weights takes about 30 minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0pHd3M6CE3e"
   },
   "source": [
    "### Enter your answers here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Frozen Embeddings**  \n",
    "random weights = 0.5618327402135231\n",
    "\n",
    "pretrained weights = 0.6650355871886121\n",
    "\n",
    "\n",
    "**Explain your results here**\n",
    "\n",
    " * The validation accuracy in case of the BERT model is closer to the validation accuracy of the ELMo embedding models, which is much higher than those of the GLoVE embedding models. This is expected as the ELMo and GloVE models consider the distribution of the contexts in the data which GloVE embedding models do not.\n",
    " * As seen from the training times and the number of epoch for GloVE, BERT, and ELMo, it can be said that ELMo takes the longest time to train and GloVE models are the fastest while BERT is in between them. \n",
    " * We can also say that randomly initialized weights in ELMo trains much slower than the case of BERT as BERT can be trained in parallel with the attention mechanism in place where as ELMo has to be trained sequentially. GloVE models are the fastest as the next word is predicted only based on the probability distribution of the entire vocabulary with no contexts in mind.\n",
    " * In the case of randomly initialized weights for ELMo and BERT, they take similar times to train as compared to the case of pretrained weights where the BERT model is slower to train than ELMo model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REeaWr4ROS9P"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'embedding_type': 'bert',\n",
    "    'embedding_size': 768,\n",
    "    'urls': [],\n",
    "    'tunable': False,\n",
    "    'batch_size': 100,\n",
    "    'num_epochs': 100,\n",
    "    'reset_weights': True\n",
    "}\n",
    "run_experiment(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKxB-wFlN4ec"
   },
   "source": [
    "###  Q3.5 - Sentence Composition (5 points)\n",
    "\n",
    "One drawback of our analysis was the word embedding aggregation algorithm (averaging) used to build the sentence embedding was lossy. Is there a better way to aggregate sentences to avoid being lossy and improve the performance of frozen embedding models? (Please give at least three solutions to get full credits)\n",
    "\n",
    "### Enter your answer here\n",
    "\n",
    "* We can use a Smooth Inverse Frequency (SIF) relation where we compute the weighted average of the word vectors in the sentence and then mask the projections of the average vector on the first singular vector.\n",
    "* A TF-IDF weighting scheme can also be used to reduce the lossy behavior and improve the model's performance.\n",
    "* Another way is to concatenate the word-embeddings and apply a linear function layer on it to further improve its performance.\n",
    "* Max-pooling is another technique one can apply to the word-embeddings, which ensemble the word embeddings with lower loss compared to the averaging of the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFWBVj-QGumb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS685_hw2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
